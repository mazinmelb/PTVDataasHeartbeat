{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Melb PTV Datathon 2018: Data Wrangling (Flinders vers)\n",
    "\n",
    "\n",
    "Intro:\n",
    "Just grab the parent route data from the card data and do some simple visualisations\n",
    "\n",
    "\n",
    "Date: 25/9/2018\n",
    "\n",
    "Version: 0.11\n",
    "\n",
    "Environment: Python 3.5.2 and Jupyter notebook\n",
    "\n",
    "Libraries used: \n",
    "    pandas, re, string, os, gzip, numpy, re, datetime, seaborn, matplotlibe, geopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## quick links\n",
    "- <a href='#import2'>Import files and libraries</a>\n",
    "- <a href='#hourly1'>look at frequencies on route by hour of day</a>\n",
    "- <a href='#window'>Create frequency table for time windows of 30 mins</a>\n",
    "- <a href='#normalise'>Normalise frequencies for unity display</a>\n",
    "- <a href='#file'>Save to file</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Import datafiles and python libraries<a id='import2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries needed to read and report on data files\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import os \n",
    "import gzip\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "# import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.dates as mdates\n",
    "import geopy.distance as distance\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm\n",
    "import peakutils\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code is working for this filtering\n",
    "myYears = ['2018']\n",
    "# search for files from root directory down\n",
    "mypath = \"./data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dfStops dataframe has (33584, 14) rows and columns with the following column names and data types.\n"
     ]
    }
   ],
   "source": [
    "colNames= ('StopID', 'SourceSystem', 'name','location','StopType','Suburb','PostCode','City','Council','Region','Lat','Long','LatTolerance', 'LongTolerance')\n",
    "dfStops = pd.DataFrame()\n",
    "with open(\"new_Stop_Data.txt\",'r',encoding = \"ISO-8859-1\") as f:\n",
    "    dfStops = pd.read_table(f, delimiter='\\t', names = colNames, dtype={'StopID':np.int64,  'Region':'category'})\n",
    "    print('The dfStops dataframe has ' + str(dfStops.shape) + ' rows and columns with the following column names and data types.')\n",
    "    dfStops.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    33584.000000\n",
       "mean       176.100316\n",
       "std       1310.931590\n",
       "min          0.000000\n",
       "25%         13.800436\n",
       "50%         26.472064\n",
       "75%         64.190996\n",
       "max      14499.815647\n",
       "Name: cDist, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get lat long of flinders st station\n",
    "cLat = dfStops[dfStops['StopID']==19854].Lat.unique()\n",
    "cLong= dfStops[dfStops['StopID']==19854].Long.unique()\n",
    "cLatLong = [cLat[0],cLong[0]]\n",
    "\n",
    "\n",
    "# create tuple of lat, long to make manipulation easier\n",
    "dfStops['LatLong'] = dfStops[['Lat','Long']].apply(tuple,axis=1)\n",
    "dfStops['cDist']=dfStops.apply(lambda row: distance.distance(cLatLong,row['LatLong']).km,axis=1 )\n",
    "dfStops.cDist.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['17850', '17877', '18089', '18090', '18175', '18176', '19499',\n",
       "       '19516', '19685', '19854', '22238', '24117', '24119', '39073',\n",
       "       '39074', '39075', '39076', '39077', '39078', '39080', '39081',\n",
       "       '41082', '41393', '45607', '62009', '62013', '62031', '62527',\n",
       "       '62531', '62540', '62548', '62585', '62721', '63018', '64404',\n",
       "       '65053', '65054'], dtype='<U21')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# don't run this for flinders\n",
    "flindersStops = dfStops[dfStops['cDist']<.2].StopID.unique().astype(str)\n",
    "flindersStops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(flindersStops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Root</th>\n",
       "      <th>Subdir</th>\n",
       "      <th>Sample</th>\n",
       "      <th>ScanType</th>\n",
       "      <th>Year</th>\n",
       "      <th>Week</th>\n",
       "      <th>fName</th>\n",
       "      <th>FullName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>.</td>\n",
       "      <td>data</td>\n",
       "      <td>Samp_0</td>\n",
       "      <td>ScanOffTransaction</td>\n",
       "      <td>2016</td>\n",
       "      <td>Week36</td>\n",
       "      <td>QID3528705_20180713_04236_0.txt.gz</td>\n",
       "      <td>./data/Samp_0/ScanOffTransaction/2016/Week36/Q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>.</td>\n",
       "      <td>data</td>\n",
       "      <td>Samp_1</td>\n",
       "      <td>ScanOnTransaction</td>\n",
       "      <td>2018</td>\n",
       "      <td>Week14</td>\n",
       "      <td>QID3533245_20180713_40008_0.txt.gz</td>\n",
       "      <td>./data/Samp_1/ScanOnTransaction/2018/Week14/QI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>.</td>\n",
       "      <td>data</td>\n",
       "      <td>Samp_0</td>\n",
       "      <td>ScanOffTransaction</td>\n",
       "      <td>2016</td>\n",
       "      <td>Week53</td>\n",
       "      <td>QID3529275_20180713_05238_0.txt.gz</td>\n",
       "      <td>./data/Samp_0/ScanOffTransaction/2016/Week53/Q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>.</td>\n",
       "      <td>data</td>\n",
       "      <td>Samp_1</td>\n",
       "      <td>ScanOnTransaction</td>\n",
       "      <td>2018</td>\n",
       "      <td>Week9</td>\n",
       "      <td>QID3533086_20180713_35510_0.txt.gz</td>\n",
       "      <td>./data/Samp_1/ScanOnTransaction/2018/Week9/QID...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>.</td>\n",
       "      <td>data</td>\n",
       "      <td>Samp_0</td>\n",
       "      <td>ScanOnTransaction</td>\n",
       "      <td>2016</td>\n",
       "      <td>Week15</td>\n",
       "      <td>QID3531287_20180713_23539_0.txt.gz</td>\n",
       "      <td>./data/Samp_0/ScanOnTransaction/2016/Week15/QI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>.</td>\n",
       "      <td>data</td>\n",
       "      <td>Samp_1</td>\n",
       "      <td>ScanOffTransaction</td>\n",
       "      <td>2017</td>\n",
       "      <td>Week25</td>\n",
       "      <td>QID3529765_20180713_10729_0.txt.gz</td>\n",
       "      <td>./data/Samp_1/ScanOffTransaction/2017/Week25/Q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>.</td>\n",
       "      <td>data</td>\n",
       "      <td>Samp_1</td>\n",
       "      <td>ScanOnTransaction</td>\n",
       "      <td>2015</td>\n",
       "      <td>Week33</td>\n",
       "      <td>QID3530883_20180713_20948_0.txt.gz</td>\n",
       "      <td>./data/Samp_1/ScanOnTransaction/2015/Week33/QI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>.</td>\n",
       "      <td>data</td>\n",
       "      <td>Samp_1</td>\n",
       "      <td>ScanOffTransaction</td>\n",
       "      <td>2016</td>\n",
       "      <td>Week35</td>\n",
       "      <td>QID3528692_20180713_04200_0.txt.gz</td>\n",
       "      <td>./data/Samp_1/ScanOffTransaction/2016/Week35/Q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>.</td>\n",
       "      <td>data</td>\n",
       "      <td>Samp_0</td>\n",
       "      <td>ScanOnTransaction</td>\n",
       "      <td>2017</td>\n",
       "      <td>Week32</td>\n",
       "      <td>QID3532464_20180713_33128_0.txt.gz</td>\n",
       "      <td>./data/Samp_0/ScanOnTransaction/2017/Week32/QI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>.</td>\n",
       "      <td>data</td>\n",
       "      <td>Samp_1</td>\n",
       "      <td>ScanOnTransaction</td>\n",
       "      <td>2018</td>\n",
       "      <td>Week19</td>\n",
       "      <td>QID3533376_20180713_40515_0.txt.gz</td>\n",
       "      <td>./data/Samp_1/ScanOnTransaction/2018/Week19/QI...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Root Subdir  Sample            ScanType  Year    Week  \\\n",
       "55     .   data  Samp_0  ScanOffTransaction  2016  Week36   \n",
       "607    .   data  Samp_1   ScanOnTransaction  2018  Week14   \n",
       "74     .   data  Samp_0  ScanOffTransaction  2016  Week53   \n",
       "627    .   data  Samp_1   ScanOnTransaction  2018   Week9   \n",
       "189    .   data  Samp_0   ScanOnTransaction  2016  Week15   \n",
       "410    .   data  Samp_1  ScanOffTransaction  2017  Week25   \n",
       "477    .   data  Samp_1   ScanOnTransaction  2015  Week33   \n",
       "368    .   data  Samp_1  ScanOffTransaction  2016  Week35   \n",
       "261    .   data  Samp_0   ScanOnTransaction  2017  Week32   \n",
       "612    .   data  Samp_1   ScanOnTransaction  2018  Week19   \n",
       "\n",
       "                                  fName  \\\n",
       "55   QID3528705_20180713_04236_0.txt.gz   \n",
       "607  QID3533245_20180713_40008_0.txt.gz   \n",
       "74   QID3529275_20180713_05238_0.txt.gz   \n",
       "627  QID3533086_20180713_35510_0.txt.gz   \n",
       "189  QID3531287_20180713_23539_0.txt.gz   \n",
       "410  QID3529765_20180713_10729_0.txt.gz   \n",
       "477  QID3530883_20180713_20948_0.txt.gz   \n",
       "368  QID3528692_20180713_04200_0.txt.gz   \n",
       "261  QID3532464_20180713_33128_0.txt.gz   \n",
       "612  QID3533376_20180713_40515_0.txt.gz   \n",
       "\n",
       "                                              FullName  \n",
       "55   ./data/Samp_0/ScanOffTransaction/2016/Week36/Q...  \n",
       "607  ./data/Samp_1/ScanOnTransaction/2018/Week14/QI...  \n",
       "74   ./data/Samp_0/ScanOffTransaction/2016/Week53/Q...  \n",
       "627  ./data/Samp_1/ScanOnTransaction/2018/Week9/QID...  \n",
       "189  ./data/Samp_0/ScanOnTransaction/2016/Week15/QI...  \n",
       "410  ./data/Samp_1/ScanOffTransaction/2017/Week25/Q...  \n",
       "477  ./data/Samp_1/ScanOnTransaction/2015/Week33/QI...  \n",
       "368  ./data/Samp_1/ScanOffTransaction/2016/Week35/Q...  \n",
       "261  ./data/Samp_0/ScanOnTransaction/2017/Week32/QI...  \n",
       "612  ./data/Samp_1/ScanOnTransaction/2018/Week19/QI...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Walk through directories to collate all  gz (compressed) filenames\n",
    "\n",
    "fnames = []\n",
    "fullnames = []\n",
    "for root, dirs,files in os.walk(mypath):\n",
    "    for f in files:\n",
    "        if \"gz\" in f:\n",
    "            fnames.append(os.path.join(root,f).split('/'))\n",
    "            fullnames.append(os.path.join(root,f)) \n",
    "\n",
    "# put files names in a dataframe\n",
    "allFiles = pd.DataFrame(fnames,columns=['Root','Subdir','Sample','ScanType','Year','Week','fName'])\n",
    "allFiles['FullName'] = fullnames\n",
    "\n",
    "allFiles.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Root</th>\n",
       "      <th>Subdir</th>\n",
       "      <th>Sample</th>\n",
       "      <th>ScanType</th>\n",
       "      <th>Year</th>\n",
       "      <th>Week</th>\n",
       "      <th>fName</th>\n",
       "      <th>FullName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>114</td>\n",
       "      <td>114</td>\n",
       "      <td>114</td>\n",
       "      <td>114</td>\n",
       "      <td>114</td>\n",
       "      <td>114</td>\n",
       "      <td>114</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>50</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>.</td>\n",
       "      <td>data</td>\n",
       "      <td>Samp_1</td>\n",
       "      <td>ScanOnTransaction</td>\n",
       "      <td>2018</td>\n",
       "      <td>Week26</td>\n",
       "      <td>QID3530491_20180713_14034_0.txt.gz</td>\n",
       "      <td>./data/Samp_1/ScanOffTransaction/2018/Week20/Q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>114</td>\n",
       "      <td>114</td>\n",
       "      <td>50</td>\n",
       "      <td>57</td>\n",
       "      <td>114</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Root Subdir  Sample           ScanType  Year    Week  \\\n",
       "count   114    114     114                114   114     114   \n",
       "unique    1      1       9                  2     1      25   \n",
       "top       .   data  Samp_1  ScanOnTransaction  2018  Week26   \n",
       "freq    114    114      50                 57   114      18   \n",
       "\n",
       "                                     fName  \\\n",
       "count                                  114   \n",
       "unique                                  50   \n",
       "top     QID3530491_20180713_14034_0.txt.gz   \n",
       "freq                                     9   \n",
       "\n",
       "                                                 FullName  \n",
       "count                                                 114  \n",
       "unique                                                114  \n",
       "top     ./data/Samp_1/ScanOffTransaction/2018/Week20/Q...  \n",
       "freq                                                    1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allFiles[allFiles['Year']=='2018'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list for all weeks in specified year\n",
    "allWeeks = allFiles[allFiles['Year'].isin(myYears)].Week.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set type of scan by the filename\n",
    "allFiles['OnOff'] = allFiles['ScanType'].apply(lambda x: \"Off\" if x == \"ScanOffTransaction\" else \"On\")\n",
    "# convert week number into an number for easier manipulation\n",
    "allFiles['WeekNbr'] = allFiles['Week'].apply(lambda x: re.findall(r'\\d+',x)[0])\n",
    "allFiles['lines'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Root</th>\n",
       "      <th>Subdir</th>\n",
       "      <th>Sample</th>\n",
       "      <th>ScanType</th>\n",
       "      <th>Year</th>\n",
       "      <th>Week</th>\n",
       "      <th>fName</th>\n",
       "      <th>FullName</th>\n",
       "      <th>OnOff</th>\n",
       "      <th>WeekNbr</th>\n",
       "      <th>lines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>321</td>\n",
       "      <td>321</td>\n",
       "      <td>321</td>\n",
       "      <td>321</td>\n",
       "      <td>321</td>\n",
       "      <td>321</td>\n",
       "      <td>321</td>\n",
       "      <td>321</td>\n",
       "      <td>321</td>\n",
       "      <td>321</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>53</td>\n",
       "      <td>157</td>\n",
       "      <td>321</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>.</td>\n",
       "      <td>data</td>\n",
       "      <td>Samp_1</td>\n",
       "      <td>ScanOnTransaction</td>\n",
       "      <td>2017</td>\n",
       "      <td>Week26</td>\n",
       "      <td>QID3533466_20180713_41101_0.txt.gz</td>\n",
       "      <td>./data/Samp_1/ScanOnTransaction/2016/Week2/QID...</td>\n",
       "      <td>On</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>321</td>\n",
       "      <td>321</td>\n",
       "      <td>157</td>\n",
       "      <td>321</td>\n",
       "      <td>106</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>321</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Root Subdir  Sample           ScanType  Year    Week  \\\n",
       "count   321    321     321                321   321     321   \n",
       "unique    1      1       9                  1     4      53   \n",
       "top       .   data  Samp_1  ScanOnTransaction  2017  Week26   \n",
       "freq    321    321     157                321   106      13   \n",
       "\n",
       "                                     fName  \\\n",
       "count                                  321   \n",
       "unique                                 157   \n",
       "top     QID3533466_20180713_41101_0.txt.gz   \n",
       "freq                                     9   \n",
       "\n",
       "                                                 FullName OnOff WeekNbr  lines  \n",
       "count                                                 321   321     321    0.0  \n",
       "unique                                                321     1      53    0.0  \n",
       "top     ./data/Samp_1/ScanOnTransaction/2016/Week2/QID...    On      26    NaN  \n",
       "freq                                                    1   321      13    NaN  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allFiles[allFiles['OnOff']=='On'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "flindersStops = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Root</th>\n",
       "      <th>Subdir</th>\n",
       "      <th>Sample</th>\n",
       "      <th>ScanType</th>\n",
       "      <th>Year</th>\n",
       "      <th>Week</th>\n",
       "      <th>fName</th>\n",
       "      <th>FullName</th>\n",
       "      <th>OnOff</th>\n",
       "      <th>WeekNbr</th>\n",
       "      <th>lines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Root  Subdir  Sample  ScanType  Year  Week  fName  FullName  OnOff  \\\n",
       "count      0       0       0         0     0     0      0         0      0   \n",
       "unique     0       0       0         0     0     0      0         0      0   \n",
       "\n",
       "        WeekNbr  lines  \n",
       "count         0      0  \n",
       "unique        0      0  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allFiles[allFiles['OnOff']!='On'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only interested in scanon\n",
    "allFiles = allFiles[allFiles['OnOff']=='On'].sort_values(by='Week')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only interested in week10 for now\n",
    "allWeeks = ['Week10']\n",
    "# allWeeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSampleForWeek(filename,scan, yr,wk):\n",
    "    print('get data from file '+filename)\n",
    "    for line in gzip.open(filename,'rt'):\n",
    "        line = line.rstrip() + \"|\" + scan+'|'+yr+'|'+wk\n",
    "        datLines.append(line.split('|'))\n",
    "        \n",
    "    df= pd.DataFrame(datLines)\n",
    "    df.columns=['Mode','BusinessDate','DateTime','CardID','CardType','VehicleID','ParentRoute','RouteID','StopID','ScanType','Year','Week']\n",
    "\n",
    "    if len(flindersStops)>0:\n",
    "        df = df.loc[df['StopID'].isin(flindersStops)]\n",
    "        print(\"flinders\")\n",
    "    \n",
    "    # convert datetime to type datetime (needs to be numpy datetime64 for windowing)\n",
    "    # created new column as dodgy workaround for problem code\n",
    "    df['newDateTime']  = pd.to_datetime(df['DateTime'].astype(str), errors='coerce')\n",
    "    # convert datetime into string for unity parsing\n",
    "    df['Day']=df['newDateTime'].apply(lambda x: dt.datetime.strftime(x,\"%d-%b-%Y\"))\n",
    "    df.set_index('newDateTime',inplace=True)\n",
    "    scols = ['Day','DateTime']\n",
    "    df.sort_values(scols, inplace=True)\n",
    "    # create the time windows wtih groupby\n",
    "    # todo consider 2nd level group by parentroute\n",
    "    gcols = ['Day']\n",
    "    grp = df.groupby(gcols).resample('30min').CardID.count().reset_index()\n",
    "    grp['Week'] = wk\n",
    "    grp['Year'] = yr\n",
    "    print('sample of grp')\n",
    "    print(grp.sample(2))\n",
    "    return grp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AgggregateDataWindows(alldf,thisDate, dataType):\n",
    "\n",
    "        print(\"been given these values\")\n",
    "        print(alldf.sample(2))\n",
    "        print(\"===given\")\n",
    "    \n",
    "        # group data into 30 minute time windows\n",
    "        gcols = ['Year','Week','Time','Minutes']\n",
    "        newgrp = alldf.groupby(gcols).CardID.sum().reset_index()\n",
    "\n",
    "        newgrp.columns = ['Year','Week','Time','Minutes','Freq']\n",
    "        newgrp['Day']= thisDate\n",
    "        gcols=['Year','Week','Day','Time','Minutes','Freq']\n",
    "        newgrp = newgrp[gcols]\n",
    "\n",
    "        # normalise the frequency counts for mapping in unity\n",
    "        maxFreq = newgrp.Freq.max()\n",
    "        minFreq = newgrp.Freq.min()\n",
    "        newgrp['normFreq'] = newgrp['Freq'].apply(lambda x: (x-minFreq)/(maxFreq-minFreq))\n",
    "\n",
    "        # sort into time order\n",
    "        scols = ['Year','Week','Minutes']    \n",
    "        newgrp.sort_values(by=scols, inplace=True)\n",
    "\n",
    "        # identify peaks for audio\n",
    "        rowindices = peakutils.indexes(newgrp['normFreq'],thres=0.15,min_dist=10)\n",
    "        print(\"peaks at \"+str(rowindices))\n",
    "        newgrp['Peak']=False\n",
    "        colIndex = newgrp.columns.get_loc('Peak')\n",
    "        newgrp.iloc[rowindices,colIndex]=True\n",
    " \n",
    "        newgrp['DataType']=dataType\n",
    "    \n",
    "        # sort into time order\n",
    "        scols = ['DataType','Year','Week','Day','Minutes']\n",
    "        # put columns in expected order for unity\n",
    "        gcols = ['DataType','Year','Week','Day', 'Minutes','Time', 'Freq', 'normFreq', 'Peak']\n",
    "        newgrp = newgrp[gcols]\n",
    "\n",
    "        return newgrp.sort_values(scols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08:09 Processing 2018 week Week10 file: ./data/Samp_0/ScanOnTransaction/2018/Week10/QID3533149_20180713_35607_0.txt.gz\n",
      "get data from file ./data/Samp_0/ScanOnTransaction/2018/Week10/QID3533149_20180713_35607_0.txt.gz\n",
      "sample of grp\n",
      "             Day         newDateTime  CardID    Week  Year\n",
      "51   05-Mar-2018 2018-03-05 05:30:00     888  Week10  2018\n",
      "115  06-Mar-2018 2018-03-06 13:30:00    2915  Week10  2018\n",
      "08:09 Processing 2018 week Week10 file: ./data/Samp_1/ScanOnTransaction/2018/Week10/QID3533149_20180713_35607_0.txt.gz\n",
      "get data from file ./data/Samp_1/ScanOnTransaction/2018/Week10/QID3533149_20180713_35607_0.txt.gz\n",
      "sample of grp\n",
      "             Day         newDateTime  CardID    Week  Year\n",
      "172  07-Mar-2018 2018-03-07 18:00:00   12073  Week10  2018\n",
      "70   05-Mar-2018 2018-03-05 15:00:00   10979  Week10  2018\n",
      "grouping the weeks data across samples\n",
      "['includes days 04-Mar-2018' 'includes days 05-Mar-2018'\n",
      " 'includes days 06-Mar-2018' 'includes days 07-Mar-2018'\n",
      " 'includes days 08-Mar-2018' 'includes days 09-Mar-2018'\n",
      " 'includes days 10-Mar-2018' 'includes days 11-Mar-2018']\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "68   05-Mar-2018 2018-03-05 14:00:00    3122  Week10  2018     28.0  14:00:00\n",
      "219  08-Mar-2018 2018-03-08 17:30:00   15270  Week10  2018     35.0  17:30:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 04-Mar-2018\n",
      "been given these values\n",
      "            Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "18  04-Mar-2018 2018-03-04 13:00:00    2108  Week10  2018     26.0  13:00:00\n",
      "5   04-Mar-2018 2018-03-04 06:30:00     157  Week10  2018     13.0  06:30:00\n",
      "===given\n",
      "peaks at [22]\n",
      "group into daily data for the week: date 05-Mar-2018\n",
      "been given these values\n",
      "            Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "40  05-Mar-2018 2018-03-05 00:00:00      95  Week10  2018      0.0  00:00:00\n",
      "66  05-Mar-2018 2018-03-05 13:00:00    2847  Week10  2018     26.0  13:00:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 06-Mar-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "104  06-Mar-2018 2018-03-06 08:00:00   21026  Week10  2018     16.0  08:00:00\n",
      "96   06-Mar-2018 2018-03-06 04:00:00      10  Week10  2018      8.0  04:00:00\n",
      "===given\n",
      "peaks at [15 34]\n",
      "group into daily data for the week: date 07-Mar-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "158  07-Mar-2018 2018-03-07 11:00:00    2916  Week10  2018     22.0  11:00:00\n",
      "178  07-Mar-2018 2018-03-07 21:00:00    1279  Week10  2018     42.0  21:00:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 08-Mar-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "221  08-Mar-2018 2018-03-08 18:30:00    4011  Week10  2018     37.0  18:30:00\n",
      "218  08-Mar-2018 2018-03-08 17:00:00    9003  Week10  2018     34.0  17:00:00\n",
      "===given\n",
      "peaks at [15 34]\n",
      "group into daily data for the week: date 09-Mar-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "277  09-Mar-2018 2018-03-09 22:30:00    3476  Week10  2018     45.0  22:30:00\n",
      "274  09-Mar-2018 2018-03-09 21:00:00    3124  Week10  2018     42.0  21:00:00\n",
      "===given\n",
      "peaks at [15 34 45]\n",
      "group into daily data for the week: date 10-Mar-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "297  10-Mar-2018 2018-03-10 08:30:00    1513  Week10  2018     17.0  08:30:00\n",
      "313  10-Mar-2018 2018-03-10 16:30:00    2769  Week10  2018     33.0  16:30:00\n",
      "===given\n",
      "peaks at [22 34 46]\n",
      "group into daily data for the week: date 11-Mar-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "334  11-Mar-2018 2018-03-11 03:00:00     125  Week10  2018      6.0  03:00:00\n",
      "331  11-Mar-2018 2018-03-11 01:30:00     330  Week10  2018      3.0  01:30:00\n",
      "===given\n",
      "peaks at []\n",
      "08:09 Processing 2018 week Week11 file: ./data/Samp_0/ScanOnTransaction/2018/Week11/QID3533190_20180713_35704_0.txt.gz\n",
      "get data from file ./data/Samp_0/ScanOnTransaction/2018/Week11/QID3533190_20180713_35704_0.txt.gz\n",
      "sample of grp\n",
      "             Day         newDateTime  CardID    Week  Year\n",
      "219  15-Mar-2018 2018-03-15 17:30:00    7720  Week11  2018\n",
      "23   11-Mar-2018 2018-03-11 15:30:00    2520  Week11  2018\n",
      "08:10 Processing 2018 week Week11 file: ./data/Samp_1/ScanOnTransaction/2018/Week11/QID3533190_20180713_35704_0.txt.gz\n",
      "get data from file ./data/Samp_1/ScanOnTransaction/2018/Week11/QID3533190_20180713_35704_0.txt.gz\n",
      "sample of grp\n",
      "             Day         newDateTime  CardID    Week  Year\n",
      "10   11-Mar-2018 2018-03-11 09:00:00    2715  Week11  2018\n",
      "316  17-Mar-2018 2018-03-17 18:00:00    5335  Week11  2018\n",
      "grouping the weeks data across samples\n",
      "['includes days 11-Mar-2018' 'includes days 12-Mar-2018'\n",
      " 'includes days 13-Mar-2018' 'includes days 14-Mar-2018'\n",
      " 'includes days 15-Mar-2018' 'includes days 16-Mar-2018'\n",
      " 'includes days 17-Mar-2018' 'includes days 18-Mar-2018']\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "246  16-Mar-2018 2018-03-16 07:00:00    6258  Week11  2018     14.0  07:00:00\n",
      "27   11-Mar-2018 2018-03-11 17:30:00    2336  Week11  2018     35.0  17:30:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 11-Mar-2018\n",
      "been given these values\n",
      "            Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "6   11-Mar-2018 2018-03-11 07:00:00     327  Week11  2018     14.0  07:00:00\n",
      "34  11-Mar-2018 2018-03-11 21:00:00     869  Week11  2018     42.0  21:00:00\n",
      "===given\n",
      "peaks at [14 26 38]\n",
      "group into daily data for the week: date 12-Mar-2018\n",
      "been given these values\n",
      "            Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "52  12-Mar-2018 2018-03-12 06:00:00     497  Week11  2018     12.0  06:00:00\n",
      "43  12-Mar-2018 2018-03-12 01:30:00       4  Week11  2018      3.0  01:30:00\n",
      "===given\n",
      "peaks at [20 34 45]\n",
      "group into daily data for the week: date 13-Mar-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "99   13-Mar-2018 2018-03-13 05:30:00    1628  Week11  2018     11.0  05:30:00\n",
      "128  13-Mar-2018 2018-03-13 20:00:00    3242  Week11  2018     40.0  20:00:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 14-Mar-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "158  14-Mar-2018 2018-03-14 11:00:00    2832  Week11  2018     22.0  11:00:00\n",
      "140  14-Mar-2018 2018-03-14 02:00:00       0  Week11  2018      4.0  02:00:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 15-Mar-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "185  15-Mar-2018 2018-03-15 00:30:00      27  Week11  2018      1.0  00:30:00\n",
      "209  15-Mar-2018 2018-03-15 12:30:00    3008  Week11  2018     25.0  12:30:00\n",
      "===given\n",
      "peaks at [15 34]\n",
      "group into daily data for the week: date 16-Mar-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "245  16-Mar-2018 2018-03-16 06:30:00    6460  Week11  2018     13.0  06:30:00\n",
      "257  16-Mar-2018 2018-03-16 12:30:00    3101  Week11  2018     25.0  12:30:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 17-Mar-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "321  17-Mar-2018 2018-03-17 20:30:00    2305  Week11  2018     41.0  20:30:00\n",
      "287  17-Mar-2018 2018-03-17 03:30:00      52  Week11  2018      7.0  03:30:00\n",
      "===given\n",
      "peaks at [34 45]\n",
      "group into daily data for the week: date 18-Mar-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "328  18-Mar-2018 2018-03-18 00:00:00     413  Week11  2018      0.0  00:00:00\n",
      "329  18-Mar-2018 2018-03-18 00:30:00     279  Week11  2018      1.0  00:30:00\n",
      "===given\n",
      "peaks at []\n",
      "08:10 Processing 2018 week Week12 file: ./data/Samp_1/ScanOnTransaction/2018/Week12/QID3533203_20180713_35755_0.txt.gz\n",
      "get data from file ./data/Samp_1/ScanOnTransaction/2018/Week12/QID3533203_20180713_35755_0.txt.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample of grp\n",
      "             Day         newDateTime  CardID    Week  Year\n",
      "311  24-Mar-2018 2018-03-24 15:30:00    2326  Week12  2018\n",
      "82   19-Mar-2018 2018-03-19 21:00:00     970  Week12  2018\n",
      "08:10 Processing 2018 week Week12 file: ./data/Samp_0/ScanOnTransaction/2018/Week12/QID3533203_20180713_35755_0.txt.gz\n",
      "get data from file ./data/Samp_0/ScanOnTransaction/2018/Week12/QID3533203_20180713_35755_0.txt.gz\n",
      "sample of grp\n",
      "             Day         newDateTime  CardID    Week  Year\n",
      "3    18-Mar-2018 2018-03-18 05:30:00     227  Week12  2018\n",
      "278  23-Mar-2018 2018-03-23 23:00:00    2001  Week12  2018\n",
      "grouping the weeks data across samples\n",
      "['includes days 18-Mar-2018' 'includes days 19-Mar-2018'\n",
      " 'includes days 20-Mar-2018' 'includes days 21-Mar-2018'\n",
      " 'includes days 22-Mar-2018' 'includes days 23-Mar-2018'\n",
      " 'includes days 24-Mar-2018' 'includes days 25-Mar-2018']\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "66   19-Mar-2018 2018-03-19 13:00:00    2909  Week12  2018     26.0  13:00:00\n",
      "105  20-Mar-2018 2018-03-20 08:30:00   13665  Week12  2018     17.0  08:30:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 18-Mar-2018\n",
      "been given these values\n",
      "            Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "11  18-Mar-2018 2018-03-18 09:30:00    1359  Week12  2018     19.0  09:30:00\n",
      "2   18-Mar-2018 2018-03-18 05:00:00      68  Week12  2018     10.0  05:00:00\n",
      "===given\n",
      "peaks at [24]\n",
      "group into daily data for the week: date 19-Mar-2018\n",
      "been given these values\n",
      "            Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "84  19-Mar-2018 2018-03-19 22:00:00     566  Week12  2018     44.0  22:00:00\n",
      "51  19-Mar-2018 2018-03-19 05:30:00    1778  Week12  2018     11.0  05:30:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 20-Mar-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "114  20-Mar-2018 2018-03-20 13:00:00    2963  Week12  2018     26.0  13:00:00\n",
      "103  20-Mar-2018 2018-03-20 07:30:00   20162  Week12  2018     15.0  07:30:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 21-Mar-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "172  21-Mar-2018 2018-03-21 18:00:00    5813  Week12  2018     36.0  18:00:00\n",
      "180  21-Mar-2018 2018-03-21 22:00:00    1628  Week12  2018     44.0  22:00:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 22-Mar-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "193  22-Mar-2018 2018-03-22 04:30:00     110  Week12  2018      9.0  04:30:00\n",
      "224  22-Mar-2018 2018-03-22 20:00:00    3597  Week12  2018     40.0  20:00:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 23-Mar-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "268  23-Mar-2018 2018-03-23 18:00:00    5978  Week12  2018     36.0  18:00:00\n",
      "265  23-Mar-2018 2018-03-23 16:30:00    6646  Week12  2018     33.0  16:30:00\n",
      "===given\n",
      "peaks at [16 34 45]\n",
      "group into daily data for the week: date 24-Mar-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "318  24-Mar-2018 2018-03-24 19:00:00    4161  Week12  2018     38.0  19:00:00\n",
      "309  24-Mar-2018 2018-03-24 14:30:00    2351  Week12  2018     29.0  14:30:00\n",
      "===given\n",
      "peaks at [20 36]\n",
      "group into daily data for the week: date 25-Mar-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "335  25-Mar-2018 2018-03-25 03:30:00     109  Week12  2018      7.0  03:30:00\n",
      "332  25-Mar-2018 2018-03-25 02:00:00     264  Week12  2018      4.0  02:00:00\n",
      "===given\n",
      "peaks at []\n",
      "08:11 Processing 2018 week Week13 file: ./data/Samp_0/ScanOnTransaction/2018/Week13/QID3533230_20180713_35900_0.txt.gz\n",
      "get data from file ./data/Samp_0/ScanOnTransaction/2018/Week13/QID3533230_20180713_35900_0.txt.gz\n",
      "sample of grp\n",
      "             Day         newDateTime  CardID    Week  Year\n",
      "251  30-Mar-2018 2018-03-30 05:30:00      29  Week13  2018\n",
      "208  29-Mar-2018 2018-03-29 08:00:00    9061  Week13  2018\n",
      "08:11 Processing 2018 week Week13 file: ./data/Samp_1/ScanOnTransaction/2018/Week13/QID3533230_20180713_35900_0.txt.gz\n",
      "get data from file ./data/Samp_1/ScanOnTransaction/2018/Week13/QID3533230_20180713_35900_0.txt.gz\n",
      "sample of grp\n",
      "             Day         newDateTime  CardID    Week  Year\n",
      "185  28-Mar-2018 2018-03-28 20:30:00    2710  Week13  2018\n",
      "69   26-Mar-2018 2018-03-26 10:30:00    5161  Week13  2018\n",
      "grouping the weeks data across samples\n",
      "['includes days 01-Apr-2018' 'includes days 25-Mar-2018'\n",
      " 'includes days 26-Mar-2018' 'includes days 27-Mar-2018'\n",
      " 'includes days 28-Mar-2018' 'includes days 29-Mar-2018'\n",
      " 'includes days 30-Mar-2018' 'includes days 31-Mar-2018']\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "215  29-Mar-2018 2018-03-29 11:30:00    5341  Week13  2018     23.0  11:30:00\n",
      "216  29-Mar-2018 2018-03-29 12:00:00    6107  Week13  2018     24.0  12:00:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 01-Apr-2018\n",
      "been given these values\n",
      "           Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "2  01-Apr-2018 2018-04-01 01:00:00     302  Week13  2018      2.0  01:00:00\n",
      "5  01-Apr-2018 2018-04-01 02:30:00     140  Week13  2018      5.0  02:30:00\n",
      "===given\n",
      "peaks at [4]\n",
      "group into daily data for the week: date 25-Mar-2018\n",
      "been given these values\n",
      "            Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "43  25-Mar-2018 2018-03-25 21:30:00     467  Week13  2018     43.0  21:30:00\n",
      "10  25-Mar-2018 2018-03-25 05:00:00     138  Week13  2018     10.0  05:00:00\n",
      "===given\n",
      "peaks at [28]\n",
      "group into daily data for the week: date 26-Mar-2018\n",
      "been given these values\n",
      "            Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "54  26-Mar-2018 2018-03-26 03:00:00       1  Week13  2018      6.0  03:00:00\n",
      "87  26-Mar-2018 2018-03-26 19:30:00    1682  Week13  2018     39.0  19:30:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 27-Mar-2018\n",
      "been given these values\n",
      "            Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "99  27-Mar-2018 2018-03-27 01:30:00       0  Week13  2018      3.0  01:30:00\n",
      "98  27-Mar-2018 2018-03-27 01:00:00       2  Week13  2018      2.0  01:00:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 28-Mar-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "158  28-Mar-2018 2018-03-28 07:00:00    6338  Week13  2018     14.0  07:00:00\n",
      "180  28-Mar-2018 2018-03-28 18:00:00    5739  Week13  2018     36.0  18:00:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 29-Mar-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "215  29-Mar-2018 2018-03-29 11:30:00    5341  Week13  2018     23.0  11:30:00\n",
      "194  29-Mar-2018 2018-03-29 01:00:00       2  Week13  2018      2.0  01:00:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 30-Mar-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "267  30-Mar-2018 2018-03-30 13:30:00    1514  Week13  2018     27.0  13:30:00\n",
      "244  30-Mar-2018 2018-03-30 02:00:00      15  Week13  2018      4.0  02:00:00\n",
      "===given\n",
      "peaks at [30 43]\n",
      "group into daily data for the week: date 31-Mar-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "301  31-Mar-2018 2018-03-31 06:30:00     243  Week13  2018     13.0  06:30:00\n",
      "335  31-Mar-2018 2018-03-31 23:30:00     432  Week13  2018     47.0  23:30:00\n",
      "===given\n",
      "peaks at [32 44]\n",
      "08:12 Processing 2018 week Week14 file: ./data/Samp_1/ScanOnTransaction/2018/Week14/QID3533245_20180713_40008_0.txt.gz\n",
      "get data from file ./data/Samp_1/ScanOnTransaction/2018/Week14/QID3533245_20180713_40008_0.txt.gz\n",
      "sample of grp\n",
      "             Day         newDateTime  CardID    Week  Year\n",
      "3    01-Apr-2018 2018-04-01 05:30:00      70  Week14  2018\n",
      "207  05-Apr-2018 2018-04-05 11:30:00    2794  Week14  2018\n",
      "08:12 Processing 2018 week Week14 file: ./data/Samp_0/ScanOnTransaction/2018/Week14/QID3533245_20180713_40008_0.txt.gz\n",
      "get data from file ./data/Samp_0/ScanOnTransaction/2018/Week14/QID3533245_20180713_40008_0.txt.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample of grp\n",
      "             Day         newDateTime  CardID    Week  Year\n",
      "282  07-Apr-2018 2018-04-07 01:00:00     312  Week14  2018\n",
      "166  04-Apr-2018 2018-04-04 15:00:00    6999  Week14  2018\n",
      "grouping the weeks data across samples\n",
      "['includes days 01-Apr-2018' 'includes days 02-Apr-2018'\n",
      " 'includes days 03-Apr-2018' 'includes days 04-Apr-2018'\n",
      " 'includes days 05-Apr-2018' 'includes days 06-Apr-2018'\n",
      " 'includes days 07-Apr-2018' 'includes days 08-Apr-2018']\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "124  03-Apr-2018 2018-04-03 18:00:00    4833  Week14  2018     36.0  18:00:00\n",
      "309  07-Apr-2018 2018-04-07 14:30:00    4690  Week14  2018     29.0  14:30:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 01-Apr-2018\n",
      "been given these values\n",
      "            Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "23  01-Apr-2018 2018-04-01 15:30:00    1619  Week14  2018     31.0  15:30:00\n",
      "1   01-Apr-2018 2018-04-01 04:30:00     103  Week14  2018      9.0  04:30:00\n",
      "===given\n",
      "peaks at [26]\n",
      "group into daily data for the week: date 02-Apr-2018\n",
      "been given these values\n",
      "            Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "61  02-Apr-2018 2018-04-02 10:30:00    3117  Week14  2018     21.0  10:30:00\n",
      "86  02-Apr-2018 2018-04-02 23:00:00     230  Week14  2018     46.0  23:00:00\n",
      "===given\n",
      "peaks at [24 36]\n",
      "group into daily data for the week: date 03-Apr-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "133  03-Apr-2018 2018-04-03 22:30:00     417  Week14  2018     45.0  22:30:00\n",
      "114  03-Apr-2018 2018-04-03 13:00:00    5252  Week14  2018     26.0  13:00:00\n",
      "===given\n",
      "peaks at [15 34]\n",
      "group into daily data for the week: date 04-Apr-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "143  04-Apr-2018 2018-04-04 03:30:00       1  Week14  2018      7.0  03:30:00\n",
      "153  04-Apr-2018 2018-04-04 08:30:00    4658  Week14  2018     17.0  08:30:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 05-Apr-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "227  05-Apr-2018 2018-04-05 21:30:00    1986  Week14  2018     43.0  21:30:00\n",
      "222  05-Apr-2018 2018-04-05 19:00:00    4817  Week14  2018     38.0  19:00:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 06-Apr-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "258  06-Apr-2018 2018-04-06 13:00:00    5697  Week14  2018     26.0  13:00:00\n",
      "263  06-Apr-2018 2018-04-06 15:30:00    8330  Week14  2018     31.0  15:30:00\n",
      "===given\n",
      "peaks at [16 34 45]\n",
      "group into daily data for the week: date 07-Apr-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "292  07-Apr-2018 2018-04-07 06:00:00     295  Week14  2018     12.0  06:00:00\n",
      "289  07-Apr-2018 2018-04-07 04:30:00      55  Week14  2018      9.0  04:30:00\n",
      "===given\n",
      "peaks at [34]\n",
      "group into daily data for the week: date 08-Apr-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "328  08-Apr-2018 2018-04-08 00:00:00     362  Week14  2018      0.0  00:00:00\n",
      "334  08-Apr-2018 2018-04-08 03:00:00     172  Week14  2018      6.0  03:00:00\n",
      "===given\n",
      "peaks at []\n",
      "08:12 Processing 2018 week Week15 file: ./data/Samp_1/ScanOnTransaction/2018/Week15/QID3533261_20180713_40106_0.txt.gz\n",
      "get data from file ./data/Samp_1/ScanOnTransaction/2018/Week15/QID3533261_20180713_40106_0.txt.gz\n",
      "sample of grp\n",
      "             Day         newDateTime  CardID    Week  Year\n",
      "319  14-Apr-2018 2018-04-14 19:30:00    1211  Week15  2018\n",
      "32   08-Apr-2018 2018-04-08 20:00:00     794  Week15  2018\n",
      "08:12 Processing 2018 week Week15 file: ./data/Samp_0/ScanOnTransaction/2018/Week15/QID3533261_20180713_40106_0.txt.gz\n",
      "get data from file ./data/Samp_0/ScanOnTransaction/2018/Week15/QID3533261_20180713_40106_0.txt.gz\n",
      "sample of grp\n",
      "             Day         newDateTime  CardID    Week  Year\n",
      "208  12-Apr-2018 2018-04-12 12:00:00    5917  Week15  2018\n",
      "103  10-Apr-2018 2018-04-10 07:30:00   15350  Week15  2018\n",
      "grouping the weeks data across samples\n",
      "['includes days 08-Apr-2018' 'includes days 09-Apr-2018'\n",
      " 'includes days 10-Apr-2018' 'includes days 11-Apr-2018'\n",
      " 'includes days 12-Apr-2018' 'includes days 13-Apr-2018'\n",
      " 'includes days 14-Apr-2018' 'includes days 15-Apr-2018']\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "48   09-Apr-2018 2018-04-09 04:00:00      20  Week15  2018      8.0  04:00:00\n",
      "251  13-Apr-2018 2018-04-13 09:30:00    3135  Week15  2018     19.0  09:30:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 08-Apr-2018\n",
      "been given these values\n",
      "           Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "1  08-Apr-2018 2018-04-08 04:30:00      50  Week15  2018      9.0  04:30:00\n",
      "8  08-Apr-2018 2018-04-08 08:00:00     698  Week15  2018     16.0  08:00:00\n",
      "===given\n",
      "peaks at [24]\n",
      "group into daily data for the week: date 09-Apr-2018\n",
      "been given these values\n",
      "            Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "82  09-Apr-2018 2018-04-09 21:00:00     877  Week15  2018     42.0  21:00:00\n",
      "69  09-Apr-2018 2018-04-09 14:30:00    3129  Week15  2018     29.0  14:30:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 10-Apr-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "134  10-Apr-2018 2018-04-10 23:00:00     677  Week15  2018     46.0  23:00:00\n",
      "123  10-Apr-2018 2018-04-10 17:30:00    7112  Week15  2018     35.0  17:30:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 11-Apr-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "166  11-Apr-2018 2018-04-11 15:00:00    7928  Week15  2018     30.0  15:00:00\n",
      "153  11-Apr-2018 2018-04-11 08:30:00   11009  Week15  2018     17.0  08:30:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 12-Apr-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "200  12-Apr-2018 2018-04-12 08:00:00    7693  Week15  2018     16.0  08:00:00\n",
      "227  12-Apr-2018 2018-04-12 21:30:00    1079  Week15  2018     43.0  21:30:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 13-Apr-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "238  13-Apr-2018 2018-04-13 03:00:00       2  Week15  2018      6.0  03:00:00\n",
      "237  13-Apr-2018 2018-04-13 02:30:00       1  Week15  2018      5.0  02:30:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 14-Apr-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "298  14-Apr-2018 2018-04-14 09:00:00    1607  Week15  2018     18.0  09:00:00\n",
      "311  14-Apr-2018 2018-04-14 15:30:00    2165  Week15  2018     31.0  15:30:00\n",
      "===given\n",
      "peaks at [33 44]\n",
      "group into daily data for the week: date 15-Apr-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "329  15-Apr-2018 2018-04-15 00:30:00     441  Week15  2018      1.0  00:30:00\n",
      "335  15-Apr-2018 2018-04-15 03:30:00      68  Week15  2018      7.0  03:30:00\n",
      "===given\n",
      "peaks at [5]\n",
      "08:13 Processing 2018 week Week16 file: ./data/Samp_0/ScanOnTransaction/2018/Week16/QID3533311_20180713_40212_0.txt.gz\n",
      "get data from file ./data/Samp_0/ScanOnTransaction/2018/Week16/QID3533311_20180713_40212_0.txt.gz\n",
      "sample of grp\n",
      "             Day         newDateTime  CardID    Week  Year\n",
      "244  20-Apr-2018 2018-04-20 06:00:00    1811  Week16  2018\n",
      "330  22-Apr-2018 2018-04-22 01:00:00     205  Week16  2018\n",
      "08:13 Processing 2018 week Week16 file: ./data/Samp_1/ScanOnTransaction/2018/Week16/QID3533311_20180713_40212_0.txt.gz\n",
      "get data from file ./data/Samp_1/ScanOnTransaction/2018/Week16/QID3533311_20180713_40212_0.txt.gz\n",
      "sample of grp\n",
      "             Day         newDateTime  CardID    Week  Year\n",
      "303  21-Apr-2018 2018-04-21 11:30:00    4921  Week16  2018\n",
      "217  19-Apr-2018 2018-04-19 16:30:00   13486  Week16  2018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grouping the weeks data across samples\n",
      "['includes days 15-Apr-2018' 'includes days 16-Apr-2018'\n",
      " 'includes days 17-Apr-2018' 'includes days 18-Apr-2018'\n",
      " 'includes days 19-Apr-2018' 'includes days 20-Apr-2018'\n",
      " 'includes days 21-Apr-2018' 'includes days 22-Apr-2018']\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "165  18-Apr-2018 2018-04-18 14:30:00    3484  Week16  2018     29.0  14:30:00\n",
      "298  21-Apr-2018 2018-04-21 09:00:00    3634  Week16  2018     18.0  09:00:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 15-Apr-2018\n",
      "been given these values\n",
      "            Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "17  15-Apr-2018 2018-04-15 12:30:00    3657  Week16  2018     25.0  12:30:00\n",
      "35  15-Apr-2018 2018-04-15 21:30:00     882  Week16  2018     43.0  21:30:00\n",
      "===given\n",
      "peaks at [16 28]\n",
      "group into daily data for the week: date 16-Apr-2018\n",
      "been given these values\n",
      "            Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "64  16-Apr-2018 2018-04-16 12:00:00    5306  Week16  2018     24.0  12:00:00\n",
      "67  16-Apr-2018 2018-04-16 13:30:00    2585  Week16  2018     27.0  13:30:00\n",
      "===given\n",
      "peaks at [15 34]\n",
      "group into daily data for the week: date 17-Apr-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "109  17-Apr-2018 2018-04-17 10:30:00    2450  Week16  2018     21.0  10:30:00\n",
      "107  17-Apr-2018 2018-04-17 09:30:00    3059  Week16  2018     19.0  09:30:00\n",
      "===given\n",
      "peaks at [15 34]\n",
      "group into daily data for the week: date 18-Apr-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "147  18-Apr-2018 2018-04-18 05:30:00     899  Week16  2018     11.0  05:30:00\n",
      "149  18-Apr-2018 2018-04-18 06:30:00    6841  Week16  2018     13.0  06:30:00\n",
      "===given\n",
      "peaks at [15 34]\n",
      "group into daily data for the week: date 19-Apr-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "212  19-Apr-2018 2018-04-19 14:00:00    3269  Week16  2018     28.0  14:00:00\n",
      "216  19-Apr-2018 2018-04-19 16:00:00   12604  Week16  2018     32.0  16:00:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 20-Apr-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "276  20-Apr-2018 2018-04-20 22:00:00    1274  Week16  2018     44.0  22:00:00\n",
      "268  20-Apr-2018 2018-04-20 18:00:00   10903  Week16  2018     36.0  18:00:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 21-Apr-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "294  21-Apr-2018 2018-04-21 07:00:00     681  Week16  2018     14.0  07:00:00\n",
      "282  21-Apr-2018 2018-04-21 01:00:00     164  Week16  2018      2.0  01:00:00\n",
      "===given\n",
      "peaks at [32]\n",
      "group into daily data for the week: date 22-Apr-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "335  22-Apr-2018 2018-04-22 03:30:00     160  Week16  2018      7.0  03:30:00\n",
      "332  22-Apr-2018 2018-04-22 02:00:00     123  Week16  2018      4.0  02:00:00\n",
      "===given\n",
      "peaks at []\n",
      "08:14 Processing 2018 week Week17 file: ./data/Samp_1/ScanOnTransaction/2018/Week17/QID3533324_20180713_40317_0.txt.gz\n",
      "get data from file ./data/Samp_1/ScanOnTransaction/2018/Week17/QID3533324_20180713_40317_0.txt.gz\n",
      "sample of grp\n",
      "             Day         newDateTime  CardID    Week  Year\n",
      "48   23-Apr-2018 2018-04-23 04:00:00      17  Week17  2018\n",
      "324  28-Apr-2018 2018-04-28 22:00:00     796  Week17  2018\n",
      "08:14 Processing 2018 week Week17 file: ./data/Samp_0/ScanOnTransaction/2018/Week17/QID3533324_20180713_40317_0.txt.gz\n",
      "get data from file ./data/Samp_0/ScanOnTransaction/2018/Week17/QID3533324_20180713_40317_0.txt.gz\n",
      "sample of grp\n",
      "             Day         newDateTime  CardID    Week  Year\n",
      "259  27-Apr-2018 2018-04-27 13:30:00    6218  Week17  2018\n",
      "55   23-Apr-2018 2018-04-23 07:30:00   19296  Week17  2018\n",
      "grouping the weeks data across samples\n",
      "['includes days 22-Apr-2018' 'includes days 23-Apr-2018'\n",
      " 'includes days 24-Apr-2018' 'includes days 25-Apr-2018'\n",
      " 'includes days 26-Apr-2018' 'includes days 27-Apr-2018'\n",
      " 'includes days 28-Apr-2018' 'includes days 29-Apr-2018']\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "259  27-Apr-2018 2018-04-27 13:30:00    6218  Week17  2018     27.0  13:30:00\n",
      "16   22-Apr-2018 2018-04-22 12:00:00    1993  Week17  2018     24.0  12:00:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 22-Apr-2018\n",
      "been given these values\n",
      "            Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "32  22-Apr-2018 2018-04-22 20:00:00    1037  Week17  2018     40.0  20:00:00\n",
      "7   22-Apr-2018 2018-04-22 07:30:00     481  Week17  2018     15.0  07:30:00\n",
      "===given\n",
      "peaks at [24]\n",
      "group into daily data for the week: date 23-Apr-2018\n",
      "been given these values\n",
      "            Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "84  23-Apr-2018 2018-04-23 22:00:00    1091  Week17  2018     44.0  22:00:00\n",
      "69  23-Apr-2018 2018-04-23 14:30:00    6696  Week17  2018     29.0  14:30:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 24-Apr-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "118  24-Apr-2018 2018-04-24 15:00:00   10847  Week17  2018     30.0  15:00:00\n",
      "110  24-Apr-2018 2018-04-24 11:00:00    5582  Week17  2018     22.0  11:00:00\n",
      "===given\n",
      "peaks at [15 34]\n",
      "group into daily data for the week: date 25-Apr-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "150  25-Apr-2018 2018-04-25 07:00:00     533  Week17  2018     14.0  07:00:00\n",
      "159  25-Apr-2018 2018-04-25 11:30:00    4566  Week17  2018     23.0  11:30:00\n",
      "===given\n",
      "peaks at [36]\n",
      "group into daily data for the week: date 26-Apr-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "229  26-Apr-2018 2018-04-26 22:30:00     514  Week17  2018     45.0  22:30:00\n",
      "219  26-Apr-2018 2018-04-26 17:30:00    7081  Week17  2018     35.0  17:30:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 27-Apr-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "274  27-Apr-2018 2018-04-27 21:00:00    1383  Week17  2018     42.0  21:00:00\n",
      "232  27-Apr-2018 2018-04-27 00:00:00      91  Week17  2018      0.0  00:00:00\n",
      "===given\n",
      "peaks at [15 34]\n",
      "group into daily data for the week: date 28-Apr-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "286  28-Apr-2018 2018-04-28 03:00:00     117  Week17  2018      6.0  03:00:00\n",
      "284  28-Apr-2018 2018-04-28 02:00:00     127  Week17  2018      4.0  02:00:00\n",
      "===given\n",
      "peaks at [34]\n",
      "group into daily data for the week: date 29-Apr-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "333  29-Apr-2018 2018-04-29 02:30:00     169  Week17  2018      5.0  02:30:00\n",
      "329  29-Apr-2018 2018-04-29 00:30:00     529  Week17  2018      1.0  00:30:00\n",
      "===given\n",
      "peaks at []\n",
      "08:14 Processing 2018 week Week18 file: ./data/Samp_1/ScanOnTransaction/2018/Week18/QID3533337_20180713_40417_0.txt.gz\n",
      "get data from file ./data/Samp_1/ScanOnTransaction/2018/Week18/QID3533337_20180713_40417_0.txt.gz\n",
      "sample of grp\n",
      "             Day         newDateTime  CardID    Week  Year\n",
      "166  04-May-2018 2018-05-04 11:00:00    2549  Week18  2018\n",
      "235  05-May-2018 2018-05-05 21:30:00     765  Week18  2018\n",
      "08:15 Processing 2018 week Week18 file: ./data/Samp_0/ScanOnTransaction/2018/Week18/QID3533337_20180713_40417_0.txt.gz\n",
      "get data from file ./data/Samp_0/ScanOnTransaction/2018/Week18/QID3533337_20180713_40417_0.txt.gz\n",
      "sample of grp\n",
      "             Day         newDateTime  CardID    Week  Year\n",
      "323  30-Apr-2018 2018-04-30 17:30:00   14669  Week18  2018\n",
      "128  03-May-2018 2018-05-03 16:00:00   12301  Week18  2018\n",
      "grouping the weeks data across samples\n",
      "['includes days 01-May-2018' 'includes days 02-May-2018'\n",
      " 'includes days 03-May-2018' 'includes days 04-May-2018'\n",
      " 'includes days 05-May-2018' 'includes days 06-May-2018'\n",
      " 'includes days 29-Apr-2018' 'includes days 30-Apr-2018']\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "167  04-May-2018 2018-05-04 11:30:00    2583  Week18  2018     23.0  11:30:00\n",
      "308  30-Apr-2018 2018-04-30 10:00:00    5150  Week18  2018     20.0  10:00:00\n",
      "===given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peaks at [15 34]\n",
      "group into daily data for the week: date 01-May-2018\n",
      "been given these values\n",
      "            Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "21  01-May-2018 2018-05-01 10:30:00    2518  Week18  2018     21.0  10:30:00\n",
      "26  01-May-2018 2018-05-01 13:00:00    5754  Week18  2018     26.0  13:00:00\n",
      "===given\n",
      "peaks at [15 34]\n",
      "group into daily data for the week: date 02-May-2018\n",
      "been given these values\n",
      "            Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "54  02-May-2018 2018-05-02 03:00:00       0  Week18  2018      6.0  03:00:00\n",
      "72  02-May-2018 2018-05-02 12:00:00    5579  Week18  2018     24.0  12:00:00\n",
      "===given\n",
      "peaks at [15 34]\n",
      "group into daily data for the week: date 03-May-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "105  03-May-2018 2018-05-03 04:30:00     115  Week18  2018      9.0  04:30:00\n",
      "117  03-May-2018 2018-05-03 10:30:00    2754  Week18  2018     21.0  10:30:00\n",
      "===given\n",
      "peaks at [15 34]\n",
      "group into daily data for the week: date 04-May-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "183  04-May-2018 2018-05-04 19:30:00    1894  Week18  2018     39.0  19:30:00\n",
      "177  04-May-2018 2018-05-04 16:30:00   12700  Week18  2018     33.0  16:30:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 05-May-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "193  05-May-2018 2018-05-05 00:30:00     420  Week18  2018      1.0  00:30:00\n",
      "229  05-May-2018 2018-05-05 18:30:00    1961  Week18  2018     37.0  18:30:00\n",
      "===given\n",
      "peaks at [33 44]\n",
      "group into daily data for the week: date 06-May-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "243  06-May-2018 2018-05-06 01:30:00     141  Week18  2018      3.0  01:30:00\n",
      "245  06-May-2018 2018-05-06 02:30:00     181  Week18  2018      5.0  02:30:00\n",
      "===given\n",
      "peaks at []\n",
      "group into daily data for the week: date 29-Apr-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "276  29-Apr-2018 2018-04-29 18:00:00    6373  Week18  2018     36.0  18:00:00\n",
      "286  29-Apr-2018 2018-04-29 23:00:00     442  Week18  2018     46.0  23:00:00\n",
      "===given\n",
      "peaks at [16 28]\n",
      "group into daily data for the week: date 30-Apr-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "289  30-Apr-2018 2018-04-30 00:30:00      14  Week18  2018      1.0  00:30:00\n",
      "295  30-Apr-2018 2018-04-30 03:30:00       0  Week18  2018      7.0  03:30:00\n",
      "===given\n",
      "peaks at [15 34]\n",
      "08:15 Processing 2018 week Week19 file: ./data/Samp_0/ScanOnTransaction/2018/Week19/QID3533376_20180713_40515_0.txt.gz\n",
      "get data from file ./data/Samp_0/ScanOnTransaction/2018/Week19/QID3533376_20180713_40515_0.txt.gz\n",
      "sample of grp\n",
      "             Day         newDateTime  CardID    Week  Year\n",
      "315  12-May-2018 2018-05-12 17:30:00    2493  Week19  2018\n",
      "91   08-May-2018 2018-05-08 01:30:00       0  Week19  2018\n",
      "08:15 Processing 2018 week Week19 file: ./data/Samp_1/ScanOnTransaction/2018/Week19/QID3533376_20180713_40515_0.txt.gz\n",
      "get data from file ./data/Samp_1/ScanOnTransaction/2018/Week19/QID3533376_20180713_40515_0.txt.gz\n",
      "sample of grp\n",
      "             Day         newDateTime  CardID    Week  Year\n",
      "314  12-May-2018 2018-05-12 17:00:00    7381  Week19  2018\n",
      "198  10-May-2018 2018-05-10 07:00:00   12842  Week19  2018\n",
      "grouping the weeks data across samples\n",
      "['includes days 06-May-2018' 'includes days 07-May-2018'\n",
      " 'includes days 08-May-2018' 'includes days 09-May-2018'\n",
      " 'includes days 10-May-2018' 'includes days 11-May-2018'\n",
      " 'includes days 12-May-2018' 'includes days 13-May-2018']\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "168  09-May-2018 2018-05-09 16:00:00    6049  Week19  2018     32.0  16:00:00\n",
      "175  09-May-2018 2018-05-09 19:30:00    3321  Week19  2018     39.0  19:30:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 06-May-2018\n",
      "been given these values\n",
      "            Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "10  06-May-2018 2018-05-06 09:00:00    2546  Week19  2018     18.0  09:00:00\n",
      "14  06-May-2018 2018-05-06 11:00:00    2157  Week19  2018     22.0  11:00:00\n",
      "===given\n",
      "peaks at [24]\n",
      "group into daily data for the week: date 07-May-2018\n",
      "been given these values\n",
      "            Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "43  07-May-2018 2018-05-07 01:30:00       1  Week19  2018      3.0  01:30:00\n",
      "70  07-May-2018 2018-05-07 15:00:00   10368  Week19  2018     30.0  15:00:00\n",
      "===given\n",
      "peaks at [15 34]\n",
      "group into daily data for the week: date 08-May-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "117  08-May-2018 2018-05-08 14:30:00    3334  Week19  2018     29.0  14:30:00\n",
      "95   08-May-2018 2018-05-08 03:30:00       0  Week19  2018      7.0  03:30:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 09-May-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "141  09-May-2018 2018-05-09 02:30:00       1  Week19  2018      5.0  02:30:00\n",
      "163  09-May-2018 2018-05-09 13:30:00    5980  Week19  2018     27.0  13:30:00\n",
      "===given\n",
      "peaks at [15 34]\n",
      "group into daily data for the week: date 10-May-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "209  10-May-2018 2018-05-10 12:30:00    5241  Week19  2018     25.0  12:30:00\n",
      "225  10-May-2018 2018-05-10 20:30:00    1140  Week19  2018     41.0  20:30:00\n",
      "===given\n",
      "peaks at [15 34]\n",
      "group into daily data for the week: date 11-May-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "242  11-May-2018 2018-05-11 05:00:00     568  Week19  2018     10.0  05:00:00\n",
      "263  11-May-2018 2018-05-11 15:30:00   10899  Week19  2018     31.0  15:30:00\n",
      "===given\n",
      "peaks at [16 34 45]\n",
      "group into daily data for the week: date 12-May-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "298  12-May-2018 2018-05-12 09:00:00    3315  Week19  2018     18.0  09:00:00\n",
      "293  12-May-2018 2018-05-12 06:30:00     779  Week19  2018     13.0  06:30:00\n",
      "===given\n",
      "peaks at [34]\n",
      "group into daily data for the week: date 13-May-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "331  13-May-2018 2018-05-13 01:30:00      87  Week19  2018      3.0  01:30:00\n",
      "333  13-May-2018 2018-05-13 02:30:00      50  Week19  2018      5.0  02:30:00\n",
      "===given\n",
      "peaks at []\n",
      "08:16 Processing 2018 week Week2 file: ./data/Samp_1/ScanOnTransaction/2018/Week2/QID3532995_20180713_34925_0.txt.gz\n",
      "get data from file ./data/Samp_1/ScanOnTransaction/2018/Week2/QID3532995_20180713_34925_0.txt.gz\n",
      "sample of grp\n",
      "             Day         newDateTime  CardID   Week  Year\n",
      "209  11-Jan-2018 2018-01-11 12:30:00    2278  Week2  2018\n",
      "311  13-Jan-2018 2018-01-13 15:30:00    1956  Week2  2018\n",
      "08:16 Processing 2018 week Week2 file: ./data/Samp_0/ScanOnTransaction/2018/Week2/QID3532995_20180713_34925_0.txt.gz\n",
      "get data from file ./data/Samp_0/ScanOnTransaction/2018/Week2/QID3532995_20180713_34925_0.txt.gz\n",
      "sample of grp\n",
      "             Day         newDateTime  CardID   Week  Year\n",
      "28   07-Jan-2018 2018-01-07 18:00:00    2555  Week2  2018\n",
      "335  14-Jan-2018 2018-01-14 03:30:00     149  Week2  2018\n",
      "grouping the weeks data across samples\n",
      "['includes days 07-Jan-2018' 'includes days 08-Jan-2018'\n",
      " 'includes days 09-Jan-2018' 'includes days 10-Jan-2018'\n",
      " 'includes days 11-Jan-2018' 'includes days 12-Jan-2018'\n",
      " 'includes days 13-Jan-2018' 'includes days 14-Jan-2018']\n",
      "been given these values\n",
      "             Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "306  13-Jan-2018 2018-01-13 13:00:00    4200  Week2  2018     26.0  13:00:00\n",
      "19   07-Jan-2018 2018-01-07 13:30:00    1591  Week2  2018     27.0  13:30:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 07-Jan-2018\n",
      "been given these values\n",
      "            Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "31  07-Jan-2018 2018-01-07 19:30:00     754  Week2  2018     39.0  19:30:00\n",
      "0   07-Jan-2018 2018-01-07 04:00:00      82  Week2  2018      8.0  04:00:00\n",
      "===given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peaks at [22 34]\n",
      "group into daily data for the week: date 08-Jan-2018\n",
      "been given these values\n",
      "            Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "64  08-Jan-2018 2018-01-08 12:00:00    2099  Week2  2018     24.0  12:00:00\n",
      "79  08-Jan-2018 2018-01-08 19:30:00    1157  Week2  2018     39.0  19:30:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 09-Jan-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "102  09-Jan-2018 2018-01-09 07:00:00    8269  Week2  2018     14.0  07:00:00\n",
      "123  09-Jan-2018 2018-01-09 17:30:00   12111  Week2  2018     35.0  17:30:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 10-Jan-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "138  10-Jan-2018 2018-01-10 01:00:00       5  Week2  2018      2.0  01:00:00\n",
      "167  10-Jan-2018 2018-01-10 15:30:00    7011  Week2  2018     31.0  15:30:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 11-Jan-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "226  11-Jan-2018 2018-01-11 21:00:00    1047  Week2  2018     42.0  21:00:00\n",
      "215  11-Jan-2018 2018-01-11 15:30:00    3370  Week2  2018     31.0  15:30:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 12-Jan-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "232  12-Jan-2018 2018-01-12 00:00:00     253  Week2  2018      0.0  00:00:00\n",
      "250  12-Jan-2018 2018-01-12 09:00:00    5910  Week2  2018     18.0  09:00:00\n",
      "===given\n",
      "peaks at [16 34 45]\n",
      "group into daily data for the week: date 13-Jan-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "294  13-Jan-2018 2018-01-13 07:00:00    1199  Week2  2018     14.0  07:00:00\n",
      "280  13-Jan-2018 2018-01-13 00:00:00     325  Week2  2018      0.0  00:00:00\n",
      "===given\n",
      "peaks at [28 44]\n",
      "group into daily data for the week: date 14-Jan-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "333  14-Jan-2018 2018-01-14 02:30:00      62  Week2  2018      5.0  02:30:00\n",
      "332  14-Jan-2018 2018-01-14 02:00:00     161  Week2  2018      4.0  02:00:00\n",
      "===given\n",
      "peaks at []\n",
      "08:16 Processing 2018 week Week20 file: ./data/Samp_1/ScanOnTransaction/2018/Week20/QID3533398_20180713_40610_0.txt.gz\n",
      "get data from file ./data/Samp_1/ScanOnTransaction/2018/Week20/QID3533398_20180713_40610_0.txt.gz\n",
      "sample of grp\n",
      "             Day         newDateTime  CardID    Week  Year\n",
      "108  15-May-2018 2018-05-15 10:00:00    2631  Week20  2018\n",
      "292  19-May-2018 2018-05-19 06:00:00     295  Week20  2018\n",
      "08:17 Processing 2018 week Week20 file: ./data/Samp_0/ScanOnTransaction/2018/Week20/QID3533398_20180713_40610_0.txt.gz\n",
      "get data from file ./data/Samp_0/ScanOnTransaction/2018/Week20/QID3533398_20180713_40610_0.txt.gz\n",
      "sample of grp\n",
      "             Day         newDateTime  CardID    Week  Year\n",
      "56   14-May-2018 2018-05-14 08:00:00   19268  Week20  2018\n",
      "189  17-May-2018 2018-05-17 02:30:00       0  Week20  2018\n",
      "grouping the weeks data across samples\n",
      "['includes days 13-May-2018' 'includes days 14-May-2018'\n",
      " 'includes days 15-May-2018' 'includes days 16-May-2018'\n",
      " 'includes days 17-May-2018' 'includes days 18-May-2018'\n",
      " 'includes days 19-May-2018' 'includes days 20-May-2018']\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "148  16-May-2018 2018-05-16 06:00:00    1827  Week20  2018     12.0  06:00:00\n",
      "75   14-May-2018 2018-05-14 17:30:00    7397  Week20  2018     35.0  17:30:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 13-May-2018\n",
      "been given these values\n",
      "            Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "32  13-May-2018 2018-05-13 20:00:00    1174  Week20  2018     40.0  20:00:00\n",
      "1   13-May-2018 2018-05-13 04:30:00     112  Week20  2018      9.0  04:30:00\n",
      "===given\n",
      "peaks at [20 34]\n",
      "group into daily data for the week: date 14-May-2018\n",
      "been given these values\n",
      "            Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "60  14-May-2018 2018-05-14 10:00:00    4850  Week20  2018     20.0  10:00:00\n",
      "41  14-May-2018 2018-05-14 00:30:00      11  Week20  2018      1.0  00:30:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 15-May-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "126  15-May-2018 2018-05-15 19:00:00    2246  Week20  2018     38.0  19:00:00\n",
      "121  15-May-2018 2018-05-15 16:30:00    6566  Week20  2018     33.0  16:30:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 16-May-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "153  16-May-2018 2018-05-16 08:30:00   12651  Week20  2018     17.0  08:30:00\n",
      "145  16-May-2018 2018-05-16 04:30:00     110  Week20  2018      9.0  04:30:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 17-May-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "185  17-May-2018 2018-05-17 00:30:00      15  Week20  2018      1.0  00:30:00\n",
      "227  17-May-2018 2018-05-17 21:30:00     981  Week20  2018     43.0  21:30:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 18-May-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "261  18-May-2018 2018-05-18 14:30:00    3368  Week20  2018     29.0  14:30:00\n",
      "235  18-May-2018 2018-05-18 01:30:00       0  Week20  2018      3.0  01:30:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 19-May-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "310  19-May-2018 2018-05-19 15:00:00    5982  Week20  2018     30.0  15:00:00\n",
      "280  19-May-2018 2018-05-19 00:00:00     752  Week20  2018      0.0  00:00:00\n",
      "===given\n",
      "peaks at [30 44]\n",
      "group into daily data for the week: date 20-May-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "335  20-May-2018 2018-05-20 03:30:00     156  Week20  2018      7.0  03:30:00\n",
      "329  20-May-2018 2018-05-20 00:30:00     279  Week20  2018      1.0  00:30:00\n",
      "===given\n",
      "peaks at []\n",
      "08:17 Processing 2018 week Week21 file: ./data/Samp_1/ScanOnTransaction/2018/Week21/QID3533409_20180713_40702_0.txt.gz\n",
      "get data from file ./data/Samp_1/ScanOnTransaction/2018/Week21/QID3533409_20180713_40702_0.txt.gz\n",
      "sample of grp\n",
      "             Day         newDateTime  CardID    Week  Year\n",
      "149  23-May-2018 2018-05-23 06:30:00    3224  Week21  2018\n",
      "250  25-May-2018 2018-05-25 09:00:00    4008  Week21  2018\n",
      "08:17 Processing 2018 week Week21 file: ./data/Samp_0/ScanOnTransaction/2018/Week21/QID3533409_20180713_40702_0.txt.gz\n",
      "get data from file ./data/Samp_0/ScanOnTransaction/2018/Week21/QID3533409_20180713_40702_0.txt.gz\n",
      "sample of grp\n",
      "             Day         newDateTime  CardID    Week  Year\n",
      "281  26-May-2018 2018-05-26 00:30:00     404  Week21  2018\n",
      "306  26-May-2018 2018-05-26 13:00:00    5453  Week21  2018\n",
      "grouping the weeks data across samples\n",
      "['includes days 20-May-2018' 'includes days 21-May-2018'\n",
      " 'includes days 22-May-2018' 'includes days 23-May-2018'\n",
      " 'includes days 24-May-2018' 'includes days 25-May-2018'\n",
      " 'includes days 26-May-2018' 'includes days 27-May-2018']\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "134  22-May-2018 2018-05-22 23:00:00     624  Week21  2018     46.0  23:00:00\n",
      "101  22-May-2018 2018-05-22 06:30:00    3351  Week21  2018     13.0  06:30:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 20-May-2018\n",
      "been given these values\n",
      "            Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "22  20-May-2018 2018-05-20 15:00:00    2025  Week21  2018     30.0  15:00:00\n",
      "32  20-May-2018 2018-05-20 20:00:00    1191  Week21  2018     40.0  20:00:00\n",
      "===given\n",
      "peaks at [24]\n",
      "group into daily data for the week: date 21-May-2018\n",
      "been given these values\n",
      "            Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "51  21-May-2018 2018-05-21 05:30:00    1735  Week21  2018     11.0  05:30:00\n",
      "55  21-May-2018 2018-05-21 07:30:00   18700  Week21  2018     15.0  07:30:00\n",
      "===given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peaks at [16 34]\n",
      "group into daily data for the week: date 22-May-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "99   22-May-2018 2018-05-22 05:30:00     912  Week21  2018     11.0  05:30:00\n",
      "112  22-May-2018 2018-05-22 12:00:00    5261  Week21  2018     24.0  12:00:00\n",
      "===given\n",
      "peaks at [15 34]\n",
      "group into daily data for the week: date 23-May-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "175  23-May-2018 2018-05-23 19:30:00    3147  Week21  2018     39.0  19:30:00\n",
      "163  23-May-2018 2018-05-23 13:30:00    2913  Week21  2018     27.0  13:30:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 24-May-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "197  24-May-2018 2018-05-24 06:30:00    3092  Week21  2018     13.0  06:30:00\n",
      "212  24-May-2018 2018-05-24 14:00:00    6154  Week21  2018     28.0  14:00:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 25-May-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "252  25-May-2018 2018-05-25 10:00:00    2676  Week21  2018     20.0  10:00:00\n",
      "274  25-May-2018 2018-05-25 21:00:00    2716  Week21  2018     42.0  21:00:00\n",
      "===given\n",
      "peaks at [16 34 45]\n",
      "group into daily data for the week: date 26-May-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "316  26-May-2018 2018-05-26 18:00:00    4403  Week21  2018     36.0  18:00:00\n",
      "296  26-May-2018 2018-05-26 08:00:00    2651  Week21  2018     16.0  08:00:00\n",
      "===given\n",
      "peaks at [33]\n",
      "group into daily data for the week: date 27-May-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "328  27-May-2018 2018-05-27 00:00:00     291  Week21  2018      0.0  00:00:00\n",
      "329  27-May-2018 2018-05-27 00:30:00     448  Week21  2018      1.0  00:30:00\n",
      "===given\n",
      "peaks at []\n",
      "08:18 Processing 2018 week Week22 file: ./data/Samp_0/ScanOnTransaction/2018/Week22/QID3533420_20180713_40753_0.txt.gz\n",
      "get data from file ./data/Samp_0/ScanOnTransaction/2018/Week22/QID3533420_20180713_40753_0.txt.gz\n",
      "sample of grp\n",
      "             Day         newDateTime  CardID    Week  Year\n",
      "274  30-May-2018 2018-05-30 17:00:00    8074  Week22  2018\n",
      "299  31-May-2018 2018-05-31 05:30:00     878  Week22  2018\n",
      "08:18 Processing 2018 week Week22 file: ./data/Samp_1/ScanOnTransaction/2018/Week22/QID3533420_20180713_40753_0.txt.gz\n",
      "get data from file ./data/Samp_1/ScanOnTransaction/2018/Week22/QID3533420_20180713_40753_0.txt.gz\n",
      "sample of grp\n",
      "             Day         newDateTime  CardID    Week  Year\n",
      "266  30-May-2018 2018-05-30 13:00:00    5362  Week22  2018\n",
      "103  03-Jun-2018 2018-06-03 03:30:00     114  Week22  2018\n",
      "grouping the weeks data across samples\n",
      "['includes days 01-Jun-2018' 'includes days 02-Jun-2018'\n",
      " 'includes days 03-Jun-2018' 'includes days 27-May-2018'\n",
      " 'includes days 28-May-2018' 'includes days 29-May-2018'\n",
      " 'includes days 30-May-2018' 'includes days 31-May-2018']\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "285  30-May-2018 2018-05-30 22:30:00     804  Week22  2018     45.0  22:30:00\n",
      "84   02-Jun-2018 2018-06-02 18:00:00    5394  Week22  2018     36.0  18:00:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 01-Jun-2018\n",
      "been given these values\n",
      "            Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "7   01-Jun-2018 2018-06-01 03:30:00       0  Week22  2018      7.0  03:30:00\n",
      "38  01-Jun-2018 2018-06-01 19:00:00    4567  Week22  2018     38.0  19:00:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 02-Jun-2018\n",
      "been given these values\n",
      "            Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "83  02-Jun-2018 2018-06-02 17:30:00    5964  Week22  2018     35.0  17:30:00\n",
      "67  02-Jun-2018 2018-06-02 09:30:00    3338  Week22  2018     19.0  09:30:00\n",
      "===given\n",
      "peaks at [33 44]\n",
      "group into daily data for the week: date 03-Jun-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "100  03-Jun-2018 2018-06-03 02:00:00     154  Week22  2018      4.0  02:00:00\n",
      "101  03-Jun-2018 2018-06-03 02:30:00     134  Week22  2018      5.0  02:30:00\n",
      "===given\n",
      "peaks at []\n",
      "group into daily data for the week: date 27-May-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "109  27-May-2018 2018-05-27 06:30:00     200  Week22  2018     13.0  06:30:00\n",
      "131  27-May-2018 2018-05-27 17:30:00    1603  Week22  2018     35.0  17:30:00\n",
      "===given\n",
      "peaks at [23]\n",
      "group into daily data for the week: date 28-May-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "150  28-May-2018 2018-05-28 03:00:00       0  Week22  2018      6.0  03:00:00\n",
      "172  28-May-2018 2018-05-28 14:00:00    2619  Week22  2018     28.0  14:00:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 29-May-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "239  29-May-2018 2018-05-29 23:30:00     160  Week22  2018     47.0  23:30:00\n",
      "219  29-May-2018 2018-05-29 13:30:00    2488  Week22  2018     27.0  13:30:00\n",
      "===given\n",
      "peaks at [15 34]\n",
      "group into daily data for the week: date 30-May-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "250  30-May-2018 2018-05-30 05:00:00     328  Week22  2018     10.0  05:00:00\n",
      "285  30-May-2018 2018-05-30 22:30:00     804  Week22  2018     45.0  22:30:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 31-May-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "302  31-May-2018 2018-05-31 07:00:00   11485  Week22  2018     14.0  07:00:00\n",
      "289  31-May-2018 2018-05-31 00:30:00      11  Week22  2018      1.0  00:30:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "08:19 Processing 2018 week Week23 file: ./data/Samp_0/ScanOnTransaction/2018/Week23/QID3533431_20180713_40841_0.txt.gz\n",
      "get data from file ./data/Samp_0/ScanOnTransaction/2018/Week23/QID3533431_20180713_40841_0.txt.gz\n",
      "sample of grp\n",
      "             Day         newDateTime  CardID    Week  Year\n",
      "136  06-Jun-2018 2018-06-06 00:00:00      59  Week23  2018\n",
      "76   04-Jun-2018 2018-06-04 18:00:00    4633  Week23  2018\n",
      "08:19 Processing 2018 week Week23 file: ./data/Samp_1/ScanOnTransaction/2018/Week23/QID3533431_20180713_40841_0.txt.gz\n",
      "get data from file ./data/Samp_1/ScanOnTransaction/2018/Week23/QID3533431_20180713_40841_0.txt.gz\n",
      "sample of grp\n",
      "             Day         newDateTime  CardID    Week  Year\n",
      "209  07-Jun-2018 2018-06-07 12:30:00    5189  Week23  2018\n",
      "300  09-Jun-2018 2018-06-09 10:00:00    3886  Week23  2018\n",
      "grouping the weeks data across samples\n",
      "['includes days 03-Jun-2018' 'includes days 04-Jun-2018'\n",
      " 'includes days 05-Jun-2018' 'includes days 06-Jun-2018'\n",
      " 'includes days 07-Jun-2018' 'includes days 08-Jun-2018'\n",
      " 'includes days 09-Jun-2018' 'includes days 10-Jun-2018']\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "187  07-Jun-2018 2018-06-07 01:30:00       1  Week23  2018      3.0  01:30:00\n",
      "316  09-Jun-2018 2018-06-09 18:00:00    2039  Week23  2018     36.0  18:00:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 03-Jun-2018\n",
      "been given these values\n",
      "            Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "30  03-Jun-2018 2018-06-03 19:00:00    1733  Week23  2018     38.0  19:00:00\n",
      "15  03-Jun-2018 2018-06-03 11:30:00    2030  Week23  2018     23.0  11:30:00\n",
      "===given\n",
      "peaks at [20]\n",
      "group into daily data for the week: date 04-Jun-2018\n",
      "been given these values\n",
      "            Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "41  04-Jun-2018 2018-06-04 00:30:00       8  Week23  2018      1.0  00:30:00\n",
      "43  04-Jun-2018 2018-06-04 01:30:00       0  Week23  2018      3.0  01:30:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 05-Jun-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "125  05-Jun-2018 2018-06-05 18:30:00    3120  Week23  2018     37.0  18:30:00\n",
      "118  05-Jun-2018 2018-06-05 15:00:00    9691  Week23  2018     30.0  15:00:00\n",
      "===given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peaks at [15 34]\n",
      "group into daily data for the week: date 06-Jun-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "178  06-Jun-2018 2018-06-06 21:00:00    1932  Week23  2018     42.0  21:00:00\n",
      "149  06-Jun-2018 2018-06-06 06:30:00    6414  Week23  2018     13.0  06:30:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 07-Jun-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "227  07-Jun-2018 2018-06-07 21:30:00     873  Week23  2018     43.0  21:30:00\n",
      "219  07-Jun-2018 2018-06-07 17:30:00    6524  Week23  2018     35.0  17:30:00\n",
      "===given\n",
      "peaks at [15 34]\n",
      "group into daily data for the week: date 08-Jun-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "239  08-Jun-2018 2018-06-08 03:30:00       0  Week23  2018      7.0  03:30:00\n",
      "267  08-Jun-2018 2018-06-08 17:30:00    5695  Week23  2018     35.0  17:30:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 09-Jun-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "282  09-Jun-2018 2018-06-09 01:00:00     105  Week23  2018      2.0  01:00:00\n",
      "288  09-Jun-2018 2018-06-09 04:00:00      77  Week23  2018      8.0  04:00:00\n",
      "===given\n",
      "peaks at [22 34]\n",
      "group into daily data for the week: date 10-Jun-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "335  10-Jun-2018 2018-06-10 03:30:00     113  Week23  2018      7.0  03:30:00\n",
      "331  10-Jun-2018 2018-06-10 01:30:00     198  Week23  2018      3.0  01:30:00\n",
      "===given\n",
      "peaks at []\n",
      "08:19 Processing 2018 week Week24 file: ./data/Samp_1/ScanOnTransaction/2018/Week24/QID3533443_20180713_40930_0.txt.gz\n",
      "get data from file ./data/Samp_1/ScanOnTransaction/2018/Week24/QID3533443_20180713_40930_0.txt.gz\n",
      "sample of grp\n",
      "             Day         newDateTime  CardID    Week  Year\n",
      "112  12-Jun-2018 2018-06-12 12:00:00    2628  Week24  2018\n",
      "217  14-Jun-2018 2018-06-14 16:30:00    5921  Week24  2018\n",
      "08:19 Processing 2018 week Week24 file: ./data/Samp_0/ScanOnTransaction/2018/Week24/QID3533443_20180713_40930_0.txt.gz\n",
      "get data from file ./data/Samp_0/ScanOnTransaction/2018/Week24/QID3533443_20180713_40930_0.txt.gz\n",
      "sample of grp\n",
      "             Day         newDateTime  CardID    Week  Year\n",
      "59   11-Jun-2018 2018-06-11 09:30:00    2346  Week24  2018\n",
      "131  12-Jun-2018 2018-06-12 21:30:00    1313  Week24  2018\n",
      "grouping the weeks data across samples\n",
      "['includes days 10-Jun-2018' 'includes days 11-Jun-2018'\n",
      " 'includes days 12-Jun-2018' 'includes days 13-Jun-2018'\n",
      " 'includes days 14-Jun-2018' 'includes days 15-Jun-2018'\n",
      " 'includes days 16-Jun-2018' 'includes days 17-Jun-2018']\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "4    10-Jun-2018 2018-06-10 06:00:00     120  Week24  2018     12.0  06:00:00\n",
      "138  13-Jun-2018 2018-06-13 01:00:00       3  Week24  2018      2.0  01:00:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 10-Jun-2018\n",
      "been given these values\n",
      "            Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "35  10-Jun-2018 2018-06-10 21:30:00    1005  Week24  2018     43.0  21:30:00\n",
      "10  10-Jun-2018 2018-06-10 09:00:00    2401  Week24  2018     18.0  09:00:00\n",
      "===given\n",
      "peaks at [22]\n",
      "group into daily data for the week: date 11-Jun-2018\n",
      "been given these values\n",
      "            Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "81  11-Jun-2018 2018-06-11 20:30:00     541  Week24  2018     41.0  20:30:00\n",
      "41  11-Jun-2018 2018-06-11 00:30:00      20  Week24  2018      1.0  00:30:00\n",
      "===given\n",
      "peaks at [25 36]\n",
      "group into daily data for the week: date 12-Jun-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "125  12-Jun-2018 2018-06-12 18:30:00    5819  Week24  2018     37.0  18:30:00\n",
      "131  12-Jun-2018 2018-06-12 21:30:00    1313  Week24  2018     43.0  21:30:00\n",
      "===given\n",
      "peaks at [15 34]\n",
      "group into daily data for the week: date 13-Jun-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "183  13-Jun-2018 2018-06-13 23:30:00     169  Week24  2018     47.0  23:30:00\n",
      "140  13-Jun-2018 2018-06-13 02:00:00       0  Week24  2018      4.0  02:00:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 14-Jun-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "199  14-Jun-2018 2018-06-14 07:30:00    8614  Week24  2018     15.0  07:30:00\n",
      "208  14-Jun-2018 2018-06-14 12:00:00    2795  Week24  2018     24.0  12:00:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 15-Jun-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "232  15-Jun-2018 2018-06-15 00:00:00     194  Week24  2018      0.0  00:00:00\n",
      "271  15-Jun-2018 2018-06-15 19:30:00    1521  Week24  2018     39.0  19:30:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 16-Jun-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "312  16-Jun-2018 2018-06-16 16:00:00    2786  Week24  2018     32.0  16:00:00\n",
      "295  16-Jun-2018 2018-06-16 07:30:00    1689  Week24  2018     15.0  07:30:00\n",
      "===given\n",
      "peaks at [32 44]\n",
      "group into daily data for the week: date 17-Jun-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "328  17-Jun-2018 2018-06-17 00:00:00     343  Week24  2018      0.0  00:00:00\n",
      "334  17-Jun-2018 2018-06-17 03:00:00      60  Week24  2018      6.0  03:00:00\n",
      "===given\n",
      "peaks at []\n",
      "08:20 Processing 2018 week Week25 file: ./data/Samp_1/ScanOnTransaction/2018/Week25/QID3533455_20180713_41014_0.txt.gz\n",
      "get data from file ./data/Samp_1/ScanOnTransaction/2018/Week25/QID3533455_20180713_41014_0.txt.gz\n",
      "sample of grp\n",
      "             Day         newDateTime  CardID    Week  Year\n",
      "16   17-Jun-2018 2018-06-17 12:00:00    1322  Week25  2018\n",
      "133  19-Jun-2018 2018-06-19 22:30:00     336  Week25  2018\n",
      "08:20 Processing 2018 week Week25 file: ./data/Samp_0/ScanOnTransaction/2018/Week25/QID3533455_20180713_41014_0.txt.gz\n",
      "get data from file ./data/Samp_0/ScanOnTransaction/2018/Week25/QID3533455_20180713_41014_0.txt.gz\n",
      "sample of grp\n",
      "             Day         newDateTime  CardID    Week  Year\n",
      "313  23-Jun-2018 2018-06-23 16:30:00    4876  Week25  2018\n",
      "32   17-Jun-2018 2018-06-17 20:00:00     937  Week25  2018\n",
      "grouping the weeks data across samples\n",
      "['includes days 17-Jun-2018' 'includes days 18-Jun-2018'\n",
      " 'includes days 19-Jun-2018' 'includes days 20-Jun-2018'\n",
      " 'includes days 21-Jun-2018' 'includes days 22-Jun-2018'\n",
      " 'includes days 23-Jun-2018' 'includes days 24-Jun-2018']\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "127  19-Jun-2018 2018-06-19 19:30:00    1346  Week25  2018     39.0  19:30:00\n",
      "193  21-Jun-2018 2018-06-21 04:30:00     207  Week25  2018      9.0  04:30:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 17-Jun-2018\n",
      "been given these values\n",
      "            Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "21  17-Jun-2018 2018-06-17 14:30:00    2866  Week25  2018     29.0  14:30:00\n",
      "28  17-Jun-2018 2018-06-17 18:00:00    2079  Week25  2018     36.0  18:00:00\n",
      "===given\n",
      "peaks at [12 28]\n",
      "group into daily data for the week: date 18-Jun-2018\n",
      "been given these values\n",
      "            Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "77  18-Jun-2018 2018-06-18 18:30:00    2907  Week25  2018     37.0  18:30:00\n",
      "60  18-Jun-2018 2018-06-18 10:00:00    2184  Week25  2018     20.0  10:00:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 19-Jun-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "108  19-Jun-2018 2018-06-19 10:00:00    4533  Week25  2018     20.0  10:00:00\n",
      "100  19-Jun-2018 2018-06-19 06:00:00    3623  Week25  2018     12.0  06:00:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 20-Jun-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "148  20-Jun-2018 2018-06-20 06:00:00    3648  Week25  2018     12.0  06:00:00\n",
      "157  20-Jun-2018 2018-06-20 10:30:00    4421  Week25  2018     21.0  10:30:00\n",
      "===given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peaks at [16 34]\n",
      "group into daily data for the week: date 21-Jun-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "186  21-Jun-2018 2018-06-21 01:00:00       2  Week25  2018      2.0  01:00:00\n",
      "208  21-Jun-2018 2018-06-21 12:00:00    5038  Week25  2018     24.0  12:00:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 22-Jun-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "267  22-Jun-2018 2018-06-22 17:30:00   11882  Week25  2018     35.0  17:30:00\n",
      "263  22-Jun-2018 2018-06-22 15:30:00    5552  Week25  2018     31.0  15:30:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 23-Jun-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "306  23-Jun-2018 2018-06-23 13:00:00    4323  Week25  2018     26.0  13:00:00\n",
      "317  23-Jun-2018 2018-06-23 18:30:00    3278  Week25  2018     37.0  18:30:00\n",
      "===given\n",
      "peaks at [33 44]\n",
      "group into daily data for the week: date 24-Jun-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "328  24-Jun-2018 2018-06-24 00:00:00     607  Week25  2018      0.0  00:00:00\n",
      "335  24-Jun-2018 2018-06-24 03:30:00     149  Week25  2018      7.0  03:30:00\n",
      "===given\n",
      "peaks at []\n",
      "08:21 Processing 2018 week Week26 file: ./data/Samp_8/ScanOnTransaction/2018/Week26/QID3533466_20180713_41101_0.txt.gz\n",
      "get data from file ./data/Samp_8/ScanOnTransaction/2018/Week26/QID3533466_20180713_41101_0.txt.gz\n",
      "sample of grp\n",
      "             Day         newDateTime  CardID    Week  Year\n",
      "58   25-Jun-2018 2018-06-25 05:00:00     273  Week26  2018\n",
      "317  30-Jun-2018 2018-06-30 14:30:00    1993  Week26  2018\n",
      "08:21 Processing 2018 week Week26 file: ./data/Samp_4/ScanOnTransaction/2018/Week26/QID3533466_20180713_41101_0.txt.gz\n",
      "get data from file ./data/Samp_4/ScanOnTransaction/2018/Week26/QID3533466_20180713_41101_0.txt.gz\n",
      "sample of grp\n",
      "             Day         newDateTime  CardID    Week  Year\n",
      "198  28-Jun-2018 2018-06-28 03:00:00       1  Week26  2018\n",
      "263  29-Jun-2018 2018-06-29 11:30:00    4484  Week26  2018\n",
      "08:21 Processing 2018 week Week26 file: ./data/Samp_5/ScanOnTransaction/2018/Week26/QID3533466_20180713_41101_0.txt.gz\n",
      "get data from file ./data/Samp_5/ScanOnTransaction/2018/Week26/QID3533466_20180713_41101_0.txt.gz\n",
      "sample of grp\n",
      "             Day         newDateTime  CardID    Week  Year\n",
      "244  29-Jun-2018 2018-06-29 02:00:00       0  Week26  2018\n",
      "165  27-Jun-2018 2018-06-27 10:30:00    6466  Week26  2018\n",
      "08:22 Processing 2018 week Week26 file: ./data/Samp_0/ScanOnTransaction/2018/Week26/QID3533466_20180713_41101_0.txt.gz\n",
      "get data from file ./data/Samp_0/ScanOnTransaction/2018/Week26/QID3533466_20180713_41101_0.txt.gz\n",
      "sample of grp\n",
      "             Day         newDateTime  CardID    Week  Year\n",
      "327  30-Jun-2018 2018-06-30 19:30:00    4079  Week26  2018\n",
      "97   26-Jun-2018 2018-06-26 00:30:00      33  Week26  2018\n",
      "08:23 Processing 2018 week Week26 file: ./data/Samp_7/ScanOnTransaction/2018/Week26/QID3533466_20180713_41101_0.txt.gz\n",
      "get data from file ./data/Samp_7/ScanOnTransaction/2018/Week26/QID3533466_20180713_41101_0.txt.gz\n",
      "sample of grp\n",
      "             Day         newDateTime  CardID    Week  Year\n",
      "114  26-Jun-2018 2018-06-26 09:00:00   18322  Week26  2018\n",
      "203  28-Jun-2018 2018-06-28 05:30:00    4116  Week26  2018\n",
      "08:24 Processing 2018 week Week26 file: ./data/Samp_6/ScanOnTransaction/2018/Week26/QID3533466_20180713_41101_0.txt.gz\n",
      "get data from file ./data/Samp_6/ScanOnTransaction/2018/Week26/QID3533466_20180713_41101_0.txt.gz\n",
      "sample of grp\n",
      "             Day         newDateTime  CardID    Week  Year\n",
      "329  30-Jun-2018 2018-06-30 20:30:00    4689  Week26  2018\n",
      "242  29-Jun-2018 2018-06-29 01:00:00      18  Week26  2018\n",
      "08:25 Processing 2018 week Week26 file: ./data/Samp_2/ScanOnTransaction/2018/Week26/QID3533466_20180713_41101_0.txt.gz\n",
      "get data from file ./data/Samp_2/ScanOnTransaction/2018/Week26/QID3533466_20180713_41101_0.txt.gz\n",
      "sample of grp\n",
      "             Day         newDateTime  CardID    Week  Year\n",
      "44   24-Jun-2018 2018-06-24 22:00:00    2558  Week26  2018\n",
      "197  28-Jun-2018 2018-06-28 02:30:00       0  Week26  2018\n",
      "08:26 Processing 2018 week Week26 file: ./data/Samp_1/ScanOnTransaction/2018/Week26/QID3533466_20180713_41101_0.txt.gz\n",
      "get data from file ./data/Samp_1/ScanOnTransaction/2018/Week26/QID3533466_20180713_41101_0.txt.gz\n",
      "sample of grp\n",
      "             Day         newDateTime  CardID    Week  Year\n",
      "313  30-Jun-2018 2018-06-30 12:30:00   18298  Week26  2018\n",
      "274  29-Jun-2018 2018-06-29 17:00:00   56984  Week26  2018\n",
      "08:28 Processing 2018 week Week26 file: ./data/Samp_9/ScanOnTransaction/2018/Week26/QID3533466_20180713_41101_0.txt.gz\n",
      "get data from file ./data/Samp_9/ScanOnTransaction/2018/Week26/QID3533466_20180713_41101_0.txt.gz\n",
      "sample of grp\n",
      "             Day         newDateTime  CardID    Week  Year\n",
      "284  29-Jun-2018 2018-06-29 22:00:00    9917  Week26  2018\n",
      "243  29-Jun-2018 2018-06-29 01:30:00       2  Week26  2018\n",
      "grouping the weeks data across samples\n",
      "['includes days 01-Jul-2018' 'includes days 24-Jun-2018'\n",
      " 'includes days 25-Jun-2018' 'includes days 26-Jun-2018'\n",
      " 'includes days 27-Jun-2018' 'includes days 28-Jun-2018'\n",
      " 'includes days 29-Jun-2018' 'includes days 30-Jun-2018']\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "189  27-Jun-2018 2018-06-27 22:30:00    3397  Week26  2018     45.0  22:30:00\n",
      "273  29-Jun-2018 2018-06-29 16:30:00   27894  Week26  2018     33.0  16:30:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 01-Jul-2018\n",
      "been given these values\n",
      "           Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "3  01-Jul-2018 2018-07-01 01:30:00     322  Week26  2018      3.0  01:30:00\n",
      "7  01-Jul-2018 2018-07-01 03:30:00     584  Week26  2018      7.0  03:30:00\n",
      "===given\n",
      "peaks at [4]\n",
      "group into daily data for the week: date 24-Jun-2018\n",
      "been given these values\n",
      "            Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "16  24-Jun-2018 2018-06-24 08:00:00     608  Week26  2018     16.0  08:00:00\n",
      "22  24-Jun-2018 2018-06-24 11:00:00   12029  Week26  2018     22.0  11:00:00\n",
      "===given\n",
      "peaks at [28]\n",
      "group into daily data for the week: date 25-Jun-2018\n",
      "been given these values\n",
      "            Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "64  25-Jun-2018 2018-06-25 08:00:00   16530  Week26  2018     16.0  08:00:00\n",
      "87  25-Jun-2018 2018-06-25 19:30:00    4709  Week26  2018     39.0  19:30:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 26-Jun-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "101  26-Jun-2018 2018-06-26 02:30:00       0  Week26  2018      5.0  02:30:00\n",
      "122  26-Jun-2018 2018-06-26 13:00:00    4736  Week26  2018     26.0  13:00:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 27-Jun-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "190  27-Jun-2018 2018-06-27 23:00:00    1090  Week26  2018     46.0  23:00:00\n",
      "175  27-Jun-2018 2018-06-27 15:30:00   15827  Week26  2018     31.0  15:30:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 28-Jun-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "207  28-Jun-2018 2018-06-28 07:30:00   32583  Week26  2018     15.0  07:30:00\n",
      "208  28-Jun-2018 2018-06-28 08:00:00   76302  Week26  2018     16.0  08:00:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 29-Jun-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "246  29-Jun-2018 2018-06-29 03:00:00       1  Week26  2018      6.0  03:00:00\n",
      "280  29-Jun-2018 2018-06-29 20:00:00    5615  Week26  2018     40.0  20:00:00\n",
      "===given\n",
      "peaks at [16 34 45]\n",
      "group into daily data for the week: date 30-Jun-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID    Week  Year  Minutes      Time\n",
      "321  30-Jun-2018 2018-06-30 16:30:00   18647  Week26  2018     33.0  16:30:00\n",
      "319  30-Jun-2018 2018-06-30 15:30:00   12943  Week26  2018     31.0  15:30:00\n",
      "===given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peaks at [33 44]\n",
      "08:30 Processing 2018 week Week3 file: ./data/Samp_0/ScanOnTransaction/2018/Week3/QID3533008_20180713_35019_0.txt.gz\n",
      "get data from file ./data/Samp_0/ScanOnTransaction/2018/Week3/QID3533008_20180713_35019_0.txt.gz\n",
      "sample of grp\n",
      "             Day         newDateTime  CardID   Week  Year\n",
      "54   15-Jan-2018 2018-01-15 07:00:00    4365  Week3  2018\n",
      "118  16-Jan-2018 2018-01-16 15:00:00    3225  Week3  2018\n",
      "08:30 Processing 2018 week Week3 file: ./data/Samp_1/ScanOnTransaction/2018/Week3/QID3533008_20180713_35019_0.txt.gz\n",
      "get data from file ./data/Samp_1/ScanOnTransaction/2018/Week3/QID3533008_20180713_35019_0.txt.gz\n",
      "sample of grp\n",
      "             Day         newDateTime  CardID   Week  Year\n",
      "161  17-Jan-2018 2018-01-17 12:30:00    4794  Week3  2018\n",
      "175  17-Jan-2018 2018-01-17 19:30:00    3279  Week3  2018\n",
      "grouping the weeks data across samples\n",
      "['includes days 14-Jan-2018' 'includes days 15-Jan-2018'\n",
      " 'includes days 16-Jan-2018' 'includes days 17-Jan-2018'\n",
      " 'includes days 18-Jan-2018' 'includes days 19-Jan-2018'\n",
      " 'includes days 20-Jan-2018' 'includes days 21-Jan-2018']\n",
      "been given these values\n",
      "             Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "13   14-Jan-2018 2018-01-14 10:30:00    1517  Week3  2018     21.0  10:30:00\n",
      "212  18-Jan-2018 2018-01-18 14:00:00    4519  Week3  2018     28.0  14:00:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 14-Jan-2018\n",
      "been given these values\n",
      "            Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "30  14-Jan-2018 2018-01-14 19:00:00    2280  Week3  2018     38.0  19:00:00\n",
      "16  14-Jan-2018 2018-01-14 12:00:00    3825  Week3  2018     24.0  12:00:00\n",
      "===given\n",
      "peaks at [18 30]\n",
      "group into daily data for the week: date 15-Jan-2018\n",
      "been given these values\n",
      "            Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "75  15-Jan-2018 2018-01-15 17:30:00    6647  Week3  2018     35.0  17:30:00\n",
      "84  15-Jan-2018 2018-01-15 22:00:00    1322  Week3  2018     44.0  22:00:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 16-Jan-2018\n",
      "been given these values\n",
      "            Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "92  16-Jan-2018 2018-01-16 02:00:00       1  Week3  2018      4.0  02:00:00\n",
      "98  16-Jan-2018 2018-01-16 05:00:00     265  Week3  2018     10.0  05:00:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 17-Jan-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "183  17-Jan-2018 2018-01-17 23:30:00     758  Week3  2018     47.0  23:30:00\n",
      "179  17-Jan-2018 2018-01-17 21:30:00    1157  Week3  2018     43.0  21:30:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 18-Jan-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "205  18-Jan-2018 2018-01-18 10:30:00    4594  Week3  2018     21.0  10:30:00\n",
      "194  18-Jan-2018 2018-01-18 05:00:00     596  Week3  2018     10.0  05:00:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 19-Jan-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "238  19-Jan-2018 2018-01-19 03:00:00       1  Week3  2018      6.0  03:00:00\n",
      "241  19-Jan-2018 2018-01-19 04:30:00     174  Week3  2018      9.0  04:30:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 20-Jan-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "291  20-Jan-2018 2018-01-20 05:30:00     374  Week3  2018     11.0  05:30:00\n",
      "290  20-Jan-2018 2018-01-20 05:00:00      91  Week3  2018     10.0  05:00:00\n",
      "===given\n",
      "peaks at [22 34]\n",
      "group into daily data for the week: date 21-Jan-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "333  21-Jan-2018 2018-01-21 02:30:00     212  Week3  2018      5.0  02:30:00\n",
      "331  21-Jan-2018 2018-01-21 01:30:00     162  Week3  2018      3.0  01:30:00\n",
      "===given\n",
      "peaks at []\n",
      "08:30 Processing 2018 week Week4 file: ./data/Samp_0/ScanOnTransaction/2018/Week4/QID3533020_20180713_35103_0.txt.gz\n",
      "get data from file ./data/Samp_0/ScanOnTransaction/2018/Week4/QID3533020_20180713_35103_0.txt.gz\n",
      "sample of grp\n",
      "             Day         newDateTime  CardID   Week  Year\n",
      "18   21-Jan-2018 2018-01-21 13:00:00    1740  Week4  2018\n",
      "209  25-Jan-2018 2018-01-25 12:30:00    2642  Week4  2018\n",
      "08:30 Processing 2018 week Week4 file: ./data/Samp_1/ScanOnTransaction/2018/Week4/QID3533020_20180713_35103_0.txt.gz\n",
      "get data from file ./data/Samp_1/ScanOnTransaction/2018/Week4/QID3533020_20180713_35103_0.txt.gz\n",
      "sample of grp\n",
      "             Day         newDateTime  CardID   Week  Year\n",
      "252  26-Jan-2018 2018-01-26 10:00:00    3277  Week4  2018\n",
      "306  27-Jan-2018 2018-01-27 13:00:00    4169  Week4  2018\n",
      "grouping the weeks data across samples\n",
      "['includes days 21-Jan-2018' 'includes days 22-Jan-2018'\n",
      " 'includes days 23-Jan-2018' 'includes days 24-Jan-2018'\n",
      " 'includes days 25-Jan-2018' 'includes days 26-Jan-2018'\n",
      " 'includes days 27-Jan-2018' 'includes days 28-Jan-2018']\n",
      "been given these values\n",
      "             Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "325  27-Jan-2018 2018-01-27 22:30:00     891  Week4  2018     45.0  22:30:00\n",
      "79   22-Jan-2018 2018-01-22 19:30:00    1469  Week4  2018     39.0  19:30:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 21-Jan-2018\n",
      "been given these values\n",
      "            Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "10  21-Jan-2018 2018-01-21 09:00:00    2588  Week4  2018     18.0  09:00:00\n",
      "30  21-Jan-2018 2018-01-21 19:00:00    1093  Week4  2018     38.0  19:00:00\n",
      "===given\n",
      "peaks at [14 26]\n",
      "group into daily data for the week: date 22-Jan-2018\n",
      "been given these values\n",
      "            Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "80  22-Jan-2018 2018-01-22 20:00:00    2534  Week4  2018     40.0  20:00:00\n",
      "59  22-Jan-2018 2018-01-22 09:30:00    2647  Week4  2018     19.0  09:30:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 23-Jan-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "127  23-Jan-2018 2018-01-23 19:30:00    1550  Week4  2018     39.0  19:30:00\n",
      "133  23-Jan-2018 2018-01-23 22:30:00     991  Week4  2018     45.0  22:30:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 24-Jan-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "183  24-Jan-2018 2018-01-24 23:30:00     286  Week4  2018     47.0  23:30:00\n",
      "180  24-Jan-2018 2018-01-24 22:00:00    2136  Week4  2018     44.0  22:00:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 25-Jan-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "205  25-Jan-2018 2018-01-25 10:30:00    2583  Week4  2018     21.0  10:30:00\n",
      "212  25-Jan-2018 2018-01-25 14:00:00    2798  Week4  2018     28.0  14:00:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 26-Jan-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "275  26-Jan-2018 2018-01-26 21:30:00    1060  Week4  2018     43.0  21:30:00\n",
      "246  26-Jan-2018 2018-01-26 07:00:00     791  Week4  2018     14.0  07:00:00\n",
      "===given\n",
      "peaks at [21 34]\n",
      "group into daily data for the week: date 27-Jan-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "306  27-Jan-2018 2018-01-27 13:00:00    2131  Week4  2018     26.0  13:00:00\n",
      "295  27-Jan-2018 2018-01-27 07:30:00     805  Week4  2018     15.0  07:30:00\n",
      "===given\n",
      "peaks at [30 42]\n",
      "group into daily data for the week: date 28-Jan-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "332  28-Jan-2018 2018-01-28 02:00:00     121  Week4  2018      4.0  02:00:00\n",
      "332  28-Jan-2018 2018-01-28 02:00:00     204  Week4  2018      4.0  02:00:00\n",
      "===given\n",
      "peaks at []\n",
      "08:31 Processing 2018 week Week5 file: ./data/Samp_0/ScanOnTransaction/2018/Week5/QID3533032_20180713_35144_0.txt.gz\n",
      "get data from file ./data/Samp_0/ScanOnTransaction/2018/Week5/QID3533032_20180713_35144_0.txt.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample of grp\n",
      "             Day         newDateTime  CardID   Week  Year\n",
      "89   02-Feb-2018 2018-02-02 20:30:00    1311  Week5  2018\n",
      "143  03-Feb-2018 2018-02-03 23:30:00     525  Week5  2018\n",
      "08:31 Processing 2018 week Week5 file: ./data/Samp_1/ScanOnTransaction/2018/Week5/QID3533032_20180713_35144_0.txt.gz\n",
      "get data from file ./data/Samp_1/ScanOnTransaction/2018/Week5/QID3533032_20180713_35144_0.txt.gz\n",
      "sample of grp\n",
      "             Day         newDateTime  CardID   Week  Year\n",
      "86   02-Feb-2018 2018-02-02 19:00:00    4997  Week5  2018\n",
      "140  03-Feb-2018 2018-02-03 22:00:00    1927  Week5  2018\n",
      "grouping the weeks data across samples\n",
      "['includes days 01-Feb-2018' 'includes days 02-Feb-2018'\n",
      " 'includes days 03-Feb-2018' 'includes days 04-Feb-2018'\n",
      " 'includes days 28-Jan-2018' 'includes days 29-Jan-2018'\n",
      " 'includes days 30-Jan-2018' 'includes days 31-Jan-2018']\n",
      "been given these values\n",
      "             Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "121  03-Feb-2018 2018-02-03 12:30:00    4654  Week5  2018     25.0  12:30:00\n",
      "158  28-Jan-2018 2018-01-28 07:00:00     294  Week5  2018     14.0  07:00:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 01-Feb-2018\n",
      "been given these values\n",
      "            Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "0   01-Feb-2018 2018-02-01 00:00:00     229  Week5  2018      0.0  00:00:00\n",
      "13  01-Feb-2018 2018-02-01 06:30:00    2896  Week5  2018     13.0  06:30:00\n",
      "===given\n",
      "peaks at [15 34]\n",
      "group into daily data for the week: date 02-Feb-2018\n",
      "been given these values\n",
      "            Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "92  02-Feb-2018 2018-02-02 22:00:00    1133  Week5  2018     44.0  22:00:00\n",
      "59  02-Feb-2018 2018-02-02 05:30:00     774  Week5  2018     11.0  05:30:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 03-Feb-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "97   03-Feb-2018 2018-02-03 00:30:00     252  Week5  2018      1.0  00:30:00\n",
      "111  03-Feb-2018 2018-02-03 07:30:00     945  Week5  2018     15.0  07:30:00\n",
      "===given\n",
      "peaks at [22 34]\n",
      "group into daily data for the week: date 04-Feb-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "144  04-Feb-2018 2018-02-04 00:00:00     732  Week5  2018      0.0  00:00:00\n",
      "148  04-Feb-2018 2018-02-04 02:00:00     111  Week5  2018      4.0  02:00:00\n",
      "===given\n",
      "peaks at []\n",
      "group into daily data for the week: date 28-Jan-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "168  28-Jan-2018 2018-01-28 12:00:00    2851  Week5  2018     24.0  12:00:00\n",
      "178  28-Jan-2018 2018-01-28 17:00:00    3040  Week5  2018     34.0  17:00:00\n",
      "===given\n",
      "peaks at [14 26 38]\n",
      "group into daily data for the week: date 29-Jan-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "205  29-Jan-2018 2018-01-29 06:30:00    2758  Week5  2018     13.0  06:30:00\n",
      "195  29-Jan-2018 2018-01-29 01:30:00       0  Week5  2018      3.0  01:30:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 30-Jan-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "258  30-Jan-2018 2018-01-30 09:00:00    7187  Week5  2018     18.0  09:00:00\n",
      "247  30-Jan-2018 2018-01-30 03:30:00       2  Week5  2018      7.0  03:30:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 31-Jan-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "310  31-Jan-2018 2018-01-31 11:00:00    2235  Week5  2018     22.0  11:00:00\n",
      "324  31-Jan-2018 2018-01-31 18:00:00    5014  Week5  2018     36.0  18:00:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "08:31 Processing 2018 week Week6 file: ./data/Samp_0/ScanOnTransaction/2018/Week6/QID3533043_20180713_35231_0.txt.gz\n",
      "get data from file ./data/Samp_0/ScanOnTransaction/2018/Week6/QID3533043_20180713_35231_0.txt.gz\n",
      "sample of grp\n",
      "             Day         newDateTime  CardID   Week  Year\n",
      "224  08-Feb-2018 2018-02-08 20:00:00    1479  Week6  2018\n",
      "235  09-Feb-2018 2018-02-09 01:30:00       0  Week6  2018\n",
      "08:32 Processing 2018 week Week6 file: ./data/Samp_1/ScanOnTransaction/2018/Week6/QID3533043_20180713_35231_0.txt.gz\n",
      "get data from file ./data/Samp_1/ScanOnTransaction/2018/Week6/QID3533043_20180713_35231_0.txt.gz\n",
      "sample of grp\n",
      "             Day         newDateTime  CardID   Week  Year\n",
      "259  09-Feb-2018 2018-02-09 13:30:00    5260  Week6  2018\n",
      "330  11-Feb-2018 2018-02-11 01:00:00     419  Week6  2018\n",
      "grouping the weeks data across samples\n",
      "['includes days 04-Feb-2018' 'includes days 05-Feb-2018'\n",
      " 'includes days 06-Feb-2018' 'includes days 07-Feb-2018'\n",
      " 'includes days 08-Feb-2018' 'includes days 09-Feb-2018'\n",
      " 'includes days 10-Feb-2018' 'includes days 11-Feb-2018']\n",
      "been given these values\n",
      "             Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "300  10-Feb-2018 2018-02-10 10:00:00    1981  Week6  2018     20.0  10:00:00\n",
      "53   05-Feb-2018 2018-02-05 06:30:00    5953  Week6  2018     13.0  06:30:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 04-Feb-2018\n",
      "been given these values\n",
      "            Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "3   04-Feb-2018 2018-02-04 05:30:00     165  Week6  2018     11.0  05:30:00\n",
      "37  04-Feb-2018 2018-02-04 22:30:00     606  Week6  2018     45.0  22:30:00\n",
      "===given\n",
      "peaks at [24]\n",
      "group into daily data for the week: date 05-Feb-2018\n",
      "been given these values\n",
      "            Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "50  05-Feb-2018 2018-02-05 05:00:00     318  Week6  2018     10.0  05:00:00\n",
      "75  05-Feb-2018 2018-02-05 17:30:00    6910  Week6  2018     35.0  17:30:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 06-Feb-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "126  06-Feb-2018 2018-02-06 19:00:00    2317  Week6  2018     38.0  19:00:00\n",
      "90   06-Feb-2018 2018-02-06 01:00:00       3  Week6  2018      2.0  01:00:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 07-Feb-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "162  07-Feb-2018 2018-02-07 13:00:00    5034  Week6  2018     26.0  13:00:00\n",
      "179  07-Feb-2018 2018-02-07 21:30:00     899  Week6  2018     43.0  21:30:00\n",
      "===given\n",
      "peaks at [15 34]\n",
      "group into daily data for the week: date 08-Feb-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "187  08-Feb-2018 2018-02-08 01:30:00       0  Week6  2018      3.0  01:30:00\n",
      "211  08-Feb-2018 2018-02-08 13:30:00    5025  Week6  2018     27.0  13:30:00\n",
      "===given\n",
      "peaks at [15 34]\n",
      "group into daily data for the week: date 09-Feb-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "267  09-Feb-2018 2018-02-09 17:30:00   12457  Week6  2018     35.0  17:30:00\n",
      "246  09-Feb-2018 2018-02-09 07:00:00   11231  Week6  2018     14.0  07:00:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 10-Feb-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "286  10-Feb-2018 2018-02-10 03:00:00      62  Week6  2018      6.0  03:00:00\n",
      "315  10-Feb-2018 2018-02-10 17:30:00    2738  Week6  2018     35.0  17:30:00\n",
      "===given\n",
      "peaks at [24 36]\n",
      "group into daily data for the week: date 11-Feb-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "334  11-Feb-2018 2018-02-11 03:00:00     110  Week6  2018      6.0  03:00:00\n",
      "331  11-Feb-2018 2018-02-11 01:30:00     336  Week6  2018      3.0  01:30:00\n",
      "===given\n",
      "peaks at []\n",
      "08:32 Processing 2018 week Week7 file: ./data/Samp_0/ScanOnTransaction/2018/Week7/QID3533054_20180713_35321_0.txt.gz\n",
      "get data from file ./data/Samp_0/ScanOnTransaction/2018/Week7/QID3533054_20180713_35321_0.txt.gz\n",
      "sample of grp\n",
      "             Day         newDateTime  CardID   Week  Year\n",
      "156  14-Feb-2018 2018-02-14 10:00:00    2376  Week7  2018\n",
      "92   13-Feb-2018 2018-02-13 02:00:00       0  Week7  2018\n",
      "08:32 Processing 2018 week Week7 file: ./data/Samp_1/ScanOnTransaction/2018/Week7/QID3533054_20180713_35321_0.txt.gz\n",
      "get data from file ./data/Samp_1/ScanOnTransaction/2018/Week7/QID3533054_20180713_35321_0.txt.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample of grp\n",
      "             Day         newDateTime  CardID   Week  Year\n",
      "246  16-Feb-2018 2018-02-16 07:00:00   11303  Week7  2018\n",
      "21   11-Feb-2018 2018-02-11 14:30:00    4357  Week7  2018\n",
      "grouping the weeks data across samples\n",
      "['includes days 11-Feb-2018' 'includes days 12-Feb-2018'\n",
      " 'includes days 13-Feb-2018' 'includes days 14-Feb-2018'\n",
      " 'includes days 15-Feb-2018' 'includes days 16-Feb-2018'\n",
      " 'includes days 17-Feb-2018' 'includes days 18-Feb-2018']\n",
      "been given these values\n",
      "             Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "228  15-Feb-2018 2018-02-15 22:00:00    1771  Week7  2018     44.0  22:00:00\n",
      "27   11-Feb-2018 2018-02-11 17:30:00    4017  Week7  2018     35.0  17:30:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 11-Feb-2018\n",
      "been given these values\n",
      "            Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "31  11-Feb-2018 2018-02-11 19:30:00    2171  Week7  2018     39.0  19:30:00\n",
      "38  11-Feb-2018 2018-02-11 23:00:00     537  Week7  2018     46.0  23:00:00\n",
      "===given\n",
      "peaks at [22 38]\n",
      "group into daily data for the week: date 12-Feb-2018\n",
      "been given these values\n",
      "            Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "83  12-Feb-2018 2018-02-12 21:30:00    1344  Week7  2018     43.0  21:30:00\n",
      "56  12-Feb-2018 2018-02-12 08:00:00    9469  Week7  2018     16.0  08:00:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 13-Feb-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "109  13-Feb-2018 2018-02-13 10:30:00    2198  Week7  2018     21.0  10:30:00\n",
      "98   13-Feb-2018 2018-02-13 05:00:00     327  Week7  2018     10.0  05:00:00\n",
      "===given\n",
      "peaks at [15 34]\n",
      "group into daily data for the week: date 14-Feb-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "177  14-Feb-2018 2018-02-14 20:30:00    2544  Week7  2018     41.0  20:30:00\n",
      "157  14-Feb-2018 2018-02-14 10:30:00    2323  Week7  2018     21.0  10:30:00\n",
      "===given\n",
      "peaks at [15 34]\n",
      "group into daily data for the week: date 15-Feb-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "202  15-Feb-2018 2018-02-15 09:00:00    3962  Week7  2018     18.0  09:00:00\n",
      "194  15-Feb-2018 2018-02-15 05:00:00     636  Week7  2018     10.0  05:00:00\n",
      "===given\n",
      "peaks at [15 34]\n",
      "group into daily data for the week: date 16-Feb-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "248  16-Feb-2018 2018-02-16 08:00:00    8823  Week7  2018     16.0  08:00:00\n",
      "244  16-Feb-2018 2018-02-16 06:00:00    3316  Week7  2018     12.0  06:00:00\n",
      "===given\n",
      "peaks at [15 34]\n",
      "group into daily data for the week: date 17-Feb-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "310  17-Feb-2018 2018-02-17 15:00:00    2589  Week7  2018     30.0  15:00:00\n",
      "303  17-Feb-2018 2018-02-17 11:30:00    4846  Week7  2018     23.0  11:30:00\n",
      "===given\n",
      "peaks at [34]\n",
      "group into daily data for the week: date 18-Feb-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "335  18-Feb-2018 2018-02-18 03:30:00     637  Week7  2018      7.0  03:30:00\n",
      "329  18-Feb-2018 2018-02-18 00:30:00    2514  Week7  2018      1.0  00:30:00\n",
      "===given\n",
      "peaks at []\n",
      "08:33 Processing 2018 week Week8 file: ./data/Samp_0/ScanOnTransaction/2018/Week8/QID3533073_20180713_35416_0.txt.gz\n",
      "get data from file ./data/Samp_0/ScanOnTransaction/2018/Week8/QID3533073_20180713_35416_0.txt.gz\n",
      "sample of grp\n",
      "             Day         newDateTime  CardID   Week  Year\n",
      "305  24-Feb-2018 2018-02-24 12:30:00    2545  Week8  2018\n",
      "234  23-Feb-2018 2018-02-23 01:00:00       3  Week8  2018\n",
      "08:33 Processing 2018 week Week8 file: ./data/Samp_1/ScanOnTransaction/2018/Week8/QID3533073_20180713_35416_0.txt.gz\n",
      "get data from file ./data/Samp_1/ScanOnTransaction/2018/Week8/QID3533073_20180713_35416_0.txt.gz\n",
      "sample of grp\n",
      "            Day         newDateTime  CardID   Week  Year\n",
      "81  19-Feb-2018 2018-02-19 20:30:00    2022  Week8  2018\n",
      "25  18-Feb-2018 2018-02-18 16:30:00    4084  Week8  2018\n",
      "grouping the weeks data across samples\n",
      "['includes days 18-Feb-2018' 'includes days 19-Feb-2018'\n",
      " 'includes days 20-Feb-2018' 'includes days 21-Feb-2018'\n",
      " 'includes days 22-Feb-2018' 'includes days 23-Feb-2018'\n",
      " 'includes days 24-Feb-2018' 'includes days 25-Feb-2018']\n",
      "been given these values\n",
      "            Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "13  18-Feb-2018 2018-02-18 10:30:00    1902  Week8  2018     21.0  10:30:00\n",
      "16  18-Feb-2018 2018-02-18 12:00:00    3943  Week8  2018     24.0  12:00:00\n",
      "===given\n",
      "peaks at [15 34]\n",
      "group into daily data for the week: date 18-Feb-2018\n",
      "been given these values\n",
      "            Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "24  18-Feb-2018 2018-02-18 16:00:00    2193  Week8  2018     32.0  16:00:00\n",
      "38  18-Feb-2018 2018-02-18 23:00:00     259  Week8  2018     46.0  23:00:00\n",
      "===given\n",
      "peaks at [23 34]\n",
      "group into daily data for the week: date 19-Feb-2018\n",
      "been given these values\n",
      "            Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "50  19-Feb-2018 2018-02-19 05:00:00     314  Week8  2018     10.0  05:00:00\n",
      "65  19-Feb-2018 2018-02-19 12:30:00    2284  Week8  2018     25.0  12:30:00\n",
      "===given\n",
      "peaks at [15 34]\n",
      "group into daily data for the week: date 20-Feb-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "120  20-Feb-2018 2018-02-20 16:00:00    6040  Week8  2018     32.0  16:00:00\n",
      "91   20-Feb-2018 2018-02-20 01:30:00       0  Week8  2018      3.0  01:30:00\n",
      "===given\n",
      "peaks at [15 34]\n",
      "group into daily data for the week: date 21-Feb-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "175  21-Feb-2018 2018-02-21 19:30:00    1722  Week8  2018     39.0  19:30:00\n",
      "178  21-Feb-2018 2018-02-21 21:00:00    1251  Week8  2018     42.0  21:00:00\n",
      "===given\n",
      "peaks at [15 34]\n",
      "group into daily data for the week: date 22-Feb-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "226  22-Feb-2018 2018-02-22 21:00:00    2596  Week8  2018     42.0  21:00:00\n",
      "187  22-Feb-2018 2018-02-22 01:30:00       1  Week8  2018      3.0  01:30:00\n",
      "===given\n",
      "peaks at [15 34]\n",
      "group into daily data for the week: date 23-Feb-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "239  23-Feb-2018 2018-02-23 03:30:00       2  Week8  2018      7.0  03:30:00\n",
      "265  23-Feb-2018 2018-02-23 16:30:00   12481  Week8  2018     33.0  16:30:00\n",
      "===given\n",
      "peaks at [15 34]\n",
      "group into daily data for the week: date 24-Feb-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "313  24-Feb-2018 2018-02-24 16:30:00    4930  Week8  2018     33.0  16:30:00\n",
      "306  24-Feb-2018 2018-02-24 13:00:00    4797  Week8  2018     26.0  13:00:00\n",
      "===given\n",
      "peaks at [22 34]\n",
      "group into daily data for the week: date 25-Feb-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "328  25-Feb-2018 2018-02-25 00:00:00     848  Week8  2018      0.0  00:00:00\n",
      "330  25-Feb-2018 2018-02-25 01:00:00     401  Week8  2018      2.0  01:00:00\n",
      "===given\n",
      "peaks at []\n",
      "08:34 Processing 2018 week Week9 file: ./data/Samp_1/ScanOnTransaction/2018/Week9/QID3533086_20180713_35510_0.txt.gz\n",
      "get data from file ./data/Samp_1/ScanOnTransaction/2018/Week9/QID3533086_20180713_35510_0.txt.gz\n",
      "sample of grp\n",
      "            Day         newDateTime  CardID   Week  Year\n",
      "13  01-Mar-2018 2018-03-01 06:30:00    3350  Week9  2018\n",
      "64  02-Mar-2018 2018-03-02 08:00:00    9622  Week9  2018\n",
      "08:34 Processing 2018 week Week9 file: ./data/Samp_0/ScanOnTransaction/2018/Week9/QID3533086_20180713_35510_0.txt.gz\n",
      "get data from file ./data/Samp_0/ScanOnTransaction/2018/Week9/QID3533086_20180713_35510_0.txt.gz\n",
      "sample of grp\n",
      "             Day         newDateTime  CardID   Week  Year\n",
      "165  25-Feb-2018 2018-02-25 10:30:00    3604  Week9  2018\n",
      "182  25-Feb-2018 2018-02-25 19:00:00    2364  Week9  2018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grouping the weeks data across samples\n",
      "['includes days 01-Mar-2018' 'includes days 02-Mar-2018'\n",
      " 'includes days 03-Mar-2018' 'includes days 04-Mar-2018'\n",
      " 'includes days 25-Feb-2018' 'includes days 26-Feb-2018'\n",
      " 'includes days 27-Feb-2018' 'includes days 28-Feb-2018']\n",
      "been given these values\n",
      "             Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "302  28-Feb-2018 2018-02-28 07:00:00   13316  Week9  2018     14.0  07:00:00\n",
      "333  28-Feb-2018 2018-02-28 22:30:00     589  Week9  2018     45.0  22:30:00\n",
      "===given\n",
      "peaks at [15 34]\n",
      "group into daily data for the week: date 01-Mar-2018\n",
      "been given these values\n",
      "            Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "0   01-Mar-2018 2018-03-01 00:00:00     195  Week9  2018      0.0  00:00:00\n",
      "28  01-Mar-2018 2018-03-01 14:00:00    6690  Week9  2018     28.0  14:00:00\n",
      "===given\n",
      "peaks at [15 34]\n",
      "group into daily data for the week: date 02-Mar-2018\n",
      "been given these values\n",
      "            Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "59  02-Mar-2018 2018-03-02 05:30:00     889  Week9  2018     11.0  05:30:00\n",
      "73  02-Mar-2018 2018-03-02 12:30:00    5795  Week9  2018     25.0  12:30:00\n",
      "===given\n",
      "peaks at [15 34]\n",
      "group into daily data for the week: date 03-Mar-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "97   03-Mar-2018 2018-03-03 00:30:00     496  Week9  2018      1.0  00:30:00\n",
      "107  03-Mar-2018 2018-03-03 05:30:00     211  Week9  2018     11.0  05:30:00\n",
      "===given\n",
      "peaks at [34 46]\n",
      "group into daily data for the week: date 04-Mar-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "144  04-Mar-2018 2018-03-04 00:00:00     398  Week9  2018      0.0  00:00:00\n",
      "149  04-Mar-2018 2018-03-04 02:30:00     233  Week9  2018      5.0  02:30:00\n",
      "===given\n",
      "peaks at []\n",
      "group into daily data for the week: date 25-Feb-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "180  25-Feb-2018 2018-02-25 18:00:00    3475  Week9  2018     36.0  18:00:00\n",
      "153  25-Feb-2018 2018-02-25 04:30:00      72  Week9  2018      9.0  04:30:00\n",
      "===given\n",
      "peaks at [22 37]\n",
      "group into daily data for the week: date 26-Feb-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "194  26-Feb-2018 2018-02-26 01:00:00       3  Week9  2018      2.0  01:00:00\n",
      "210  26-Feb-2018 2018-02-26 09:00:00    4245  Week9  2018     18.0  09:00:00\n",
      "===given\n",
      "peaks at [16 34]\n",
      "group into daily data for the week: date 27-Feb-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "287  27-Feb-2018 2018-02-27 23:30:00     453  Week9  2018     47.0  23:30:00\n",
      "244  27-Feb-2018 2018-02-27 02:00:00       1  Week9  2018      4.0  02:00:00\n",
      "===given\n",
      "peaks at [15 34]\n",
      "group into daily data for the week: date 28-Feb-2018\n",
      "been given these values\n",
      "             Day         newDateTime  CardID   Week  Year  Minutes      Time\n",
      "323  28-Feb-2018 2018-02-28 17:30:00   15661  Week9  2018     35.0  17:30:00\n",
      "331  28-Feb-2018 2018-02-28 21:30:00    2001  Week9  2018     43.0  21:30:00\n",
      "===given\n",
      "peaks at [15 34]\n",
      "08:34\n",
      "------Finished all weeks ------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Freq</th>\n",
       "      <th>Minutes</th>\n",
       "      <th>normFreq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>384.000000</td>\n",
       "      <td>384.000000</td>\n",
       "      <td>384.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>12920.119792</td>\n",
       "      <td>23.500000</td>\n",
       "      <td>0.340286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>22918.825302</td>\n",
       "      <td>13.871473</td>\n",
       "      <td>0.309774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1227.750000</td>\n",
       "      <td>11.750000</td>\n",
       "      <td>0.061830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6523.500000</td>\n",
       "      <td>23.500000</td>\n",
       "      <td>0.279768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>12027.500000</td>\n",
       "      <td>35.250000</td>\n",
       "      <td>0.606826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>157641.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Freq     Minutes    normFreq\n",
       "count     384.000000  384.000000  384.000000\n",
       "mean    12920.119792   23.500000    0.340286\n",
       "std     22918.825302   13.871473    0.309774\n",
       "min         0.000000    0.000000    0.000000\n",
       "25%      1227.750000   11.750000    0.061830\n",
       "50%      6523.500000   23.500000    0.279768\n",
       "75%     12027.500000   35.250000    0.606826\n",
       "max    157641.000000   47.000000    1.000000"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import all scanon data files for selected year (parameter at the beginning)\n",
    "# takes about an hour\n",
    "\n",
    "for w in allWeeks:\n",
    "    # @filter\n",
    "    datLines = []\n",
    "    allGrps = pd.DataFrame()\n",
    "    for i,row in allFiles[(allFiles['Year'].isin(myYears))&(allFiles['Week']==w)].iterrows():\n",
    "        \n",
    "        print(dt.datetime.now().strftime('%H:%M')+ \n",
    "          ' Processing '+row['Year']+' week '+ row['Week']+' file: '+ row['FullName'])\n",
    "        grp = getSampleForWeek(row['FullName'],row['OnOff'],row['Year'],row['Week'])\n",
    "        allGrps = allGrps.append(grp)\n",
    "        # todo add mode to show train/tram/bus proportions\n",
    "    \n",
    "    day1 = allGrps['Day'].min()\n",
    "    daysInWeek = allGrps['Day'].unique()\n",
    "\n",
    "    # convert time into minutes after midnight\n",
    "    allGrps['Minutes']=allGrps['newDateTime'].apply(lambda x: (x-x.normalize()).total_seconds()/60/30)\n",
    "    allGrps['Time']=allGrps['newDateTime'].apply(lambda x: dt.datetime.time(x))\n",
    "    \n",
    "    print('grouping the weeks data across samples')\n",
    "    print('includes days '+allGrps['Day'].unique())\n",
    "    grp = AgggregateDataWindows(allGrps,day1,'Weekly') \n",
    "\n",
    "    for d in daysInWeek:\n",
    "        print('group into daily data for the week: date '+d)\n",
    "        dayGrp = AgggregateDataWindows(allGrps[allGrps['Day']==d],d,'Daily')\n",
    "        grp = grp.append(dayGrp,ignore_index=True,sort=True)\n",
    "\n",
    "    # sort into time order\n",
    "    scols = ['DataType','Day','Minutes']\n",
    "    colIndex = grp.columns.get_loc('Day')\n",
    "    # save week data for visualisation in unity\n",
    "    fname = \"weekly\"+str(grp.iloc[1,colIndex])+\".csv\"\n",
    "    if len(flindersStops)>0:\n",
    "        fname = \"ecgflinders\"+str(grp.iloc[1,colIndex])+\".csv\"\n",
    "        print(\"flinders\")\n",
    "    grp.sort_values(by=scols).to_csv(fname,index=False)\n",
    "    \n",
    "print(dt.datetime.now().strftime('%H:%M'))      \n",
    "print('------Finished all weeks ------')\n",
    "\n",
    "grp.describe()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['01-Mar-2018', '02-Mar-2018', '03-Mar-2018', '04-Mar-2018',\n",
       "       '25-Feb-2018', '26-Feb-2018', '27-Feb-2018', '28-Feb-2018'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grp['Day'].unique()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# todo - try to pickup school time peak\n",
    "# identify peaks for audio\n",
    "rowindices = peakutils.indexes(grp['normFreq'],thres=0.5,min_dist=.5)\n",
    "print(\"peaks at \"+str(rowindices))\n",
    "grp['Peak']=False\n",
    "colIndex = grp.columns.get_loc('Peak')\n",
    "grp.iloc[rowindices,colIndex]=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[16 34]'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(rowindices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Minutes</th>\n",
       "      <th>Freq</th>\n",
       "      <th>Day</th>\n",
       "      <th>normFreq</th>\n",
       "      <th>Peak</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>07:30:00</td>\n",
       "      <td>15.0</td>\n",
       "      <td>157641</td>\n",
       "      <td>01-Mar-2018</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>17:00:00</td>\n",
       "      <td>34.0</td>\n",
       "      <td>148554</td>\n",
       "      <td>01-Mar-2018</td>\n",
       "      <td>0.94216</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Time  Minutes    Freq          Day  normFreq  Peak\n",
       "15  07:30:00     15.0  157641  01-Mar-2018   1.00000  True\n",
       "34  17:00:00     34.0  148554  01-Mar-2018   0.94216  True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grp[grp['Peak']==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1878'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grp.iloc[1,3].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# look at pattern of cards scan on and off by hourly window\n",
    "myPlt = grp['Freq'].plot()\n",
    "plt.title('scanon frequencies by 30 min window')\n",
    "plt.ylabel('freq'), plt.xlabel('time of day');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Time', 'Minutes', 'Freq', 'Day', 'normFreq', 'Peak'], dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Minutes</th>\n",
       "      <th>CardID</th>\n",
       "      <th>Day1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01:30:00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>890</td>\n",
       "      <td>04-Mar-2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>19:00:00</td>\n",
       "      <td>38.0</td>\n",
       "      <td>50700</td>\n",
       "      <td>04-Mar-2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>09:30:00</td>\n",
       "      <td>19.0</td>\n",
       "      <td>61228</td>\n",
       "      <td>04-Mar-2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>10:00:00</td>\n",
       "      <td>20.0</td>\n",
       "      <td>56173</td>\n",
       "      <td>04-Mar-2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>10:30:00</td>\n",
       "      <td>21.0</td>\n",
       "      <td>53349</td>\n",
       "      <td>04-Mar-2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>04:00:00</td>\n",
       "      <td>8.0</td>\n",
       "      <td>570</td>\n",
       "      <td>04-Mar-2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>02:00:00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>664</td>\n",
       "      <td>04-Mar-2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>23:00:00</td>\n",
       "      <td>46.0</td>\n",
       "      <td>17530</td>\n",
       "      <td>04-Mar-2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>14:30:00</td>\n",
       "      <td>29.0</td>\n",
       "      <td>68588</td>\n",
       "      <td>04-Mar-2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>05:30:00</td>\n",
       "      <td>11.0</td>\n",
       "      <td>14110</td>\n",
       "      <td>04-Mar-2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Time  Minutes  CardID         Day1\n",
       "3   01:30:00      3.0     890  04-Mar-2018\n",
       "38  19:00:00     38.0   50700  04-Mar-2018\n",
       "19  09:30:00     19.0   61228  04-Mar-2018\n",
       "20  10:00:00     20.0   56173  04-Mar-2018\n",
       "21  10:30:00     21.0   53349  04-Mar-2018\n",
       "8   04:00:00      8.0     570  04-Mar-2018\n",
       "4   02:00:00      4.0     664  04-Mar-2018\n",
       "46  23:00:00     46.0   17530  04-Mar-2018\n",
       "29  14:30:00     29.0   68588  04-Mar-2018\n",
       "11  05:30:00     11.0   14110  04-Mar-2018"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grp.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'04-Mar-2018'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allGrps['Day'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Day</th>\n",
       "      <th>newDateTime</th>\n",
       "      <th>CardID</th>\n",
       "      <th>Minutes</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>07-Mar-2018</td>\n",
       "      <td>2018-03-07 17:00:00</td>\n",
       "      <td>9027</td>\n",
       "      <td>34.0</td>\n",
       "      <td>17:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>10-Mar-2018</td>\n",
       "      <td>2018-03-10 21:30:00</td>\n",
       "      <td>2416</td>\n",
       "      <td>43.0</td>\n",
       "      <td>21:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>04-Mar-2018</td>\n",
       "      <td>2018-03-04 15:30:00</td>\n",
       "      <td>2105</td>\n",
       "      <td>31.0</td>\n",
       "      <td>15:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>06-Mar-2018</td>\n",
       "      <td>2018-03-06 12:00:00</td>\n",
       "      <td>5880</td>\n",
       "      <td>24.0</td>\n",
       "      <td>12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>10-Mar-2018</td>\n",
       "      <td>2018-03-10 11:00:00</td>\n",
       "      <td>2571</td>\n",
       "      <td>22.0</td>\n",
       "      <td>11:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>10-Mar-2018</td>\n",
       "      <td>2018-03-10 02:00:00</td>\n",
       "      <td>171</td>\n",
       "      <td>4.0</td>\n",
       "      <td>02:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>08-Mar-2018</td>\n",
       "      <td>2018-03-08 00:00:00</td>\n",
       "      <td>105</td>\n",
       "      <td>0.0</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>09-Mar-2018</td>\n",
       "      <td>2018-03-09 10:30:00</td>\n",
       "      <td>2817</td>\n",
       "      <td>21.0</td>\n",
       "      <td>10:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>06-Mar-2018</td>\n",
       "      <td>2018-03-06 20:30:00</td>\n",
       "      <td>2886</td>\n",
       "      <td>41.0</td>\n",
       "      <td>20:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>05-Mar-2018</td>\n",
       "      <td>2018-03-05 03:30:00</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>03:30:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Day         newDateTime  CardID  Minutes      Time\n",
       "170  07-Mar-2018 2018-03-07 17:00:00    9027     34.0  17:00:00\n",
       "323  10-Mar-2018 2018-03-10 21:30:00    2416     43.0  21:30:00\n",
       "23   04-Mar-2018 2018-03-04 15:30:00    2105     31.0  15:30:00\n",
       "112  06-Mar-2018 2018-03-06 12:00:00    5880     24.0  12:00:00\n",
       "302  10-Mar-2018 2018-03-10 11:00:00    2571     22.0  11:00:00\n",
       "284  10-Mar-2018 2018-03-10 02:00:00     171      4.0  02:00:00\n",
       "184  08-Mar-2018 2018-03-08 00:00:00     105      0.0  00:00:00\n",
       "253  09-Mar-2018 2018-03-09 10:30:00    2817     21.0  10:30:00\n",
       "129  06-Mar-2018 2018-03-06 20:30:00    2886     41.0  20:30:00\n",
       "47   05-Mar-2018 2018-03-05 03:30:00       1      7.0  03:30:00"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allGrps.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo add in sunrise/sunset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Day</th>\n",
       "      <th>DateTime</th>\n",
       "      <th>Freq</th>\n",
       "      <th>normFreq</th>\n",
       "      <th>Minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>04-Mar-2018</td>\n",
       "      <td>2018-03-04 11:00:00</td>\n",
       "      <td>5617</td>\n",
       "      <td>0.176886</td>\n",
       "      <td>660.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>07-Mar-2018</td>\n",
       "      <td>2018-03-07 21:00:00</td>\n",
       "      <td>3861</td>\n",
       "      <td>0.121587</td>\n",
       "      <td>1260.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>10-Mar-2018</td>\n",
       "      <td>2018-03-10 17:00:00</td>\n",
       "      <td>9324</td>\n",
       "      <td>0.293623</td>\n",
       "      <td>1020.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>05-Mar-2018</td>\n",
       "      <td>2018-03-05 07:00:00</td>\n",
       "      <td>19634</td>\n",
       "      <td>0.618296</td>\n",
       "      <td>420.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>08-Mar-2018</td>\n",
       "      <td>2018-03-08 15:30:00</td>\n",
       "      <td>20122</td>\n",
       "      <td>0.633664</td>\n",
       "      <td>930.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>09-Mar-2018</td>\n",
       "      <td>2018-03-09 13:30:00</td>\n",
       "      <td>9482</td>\n",
       "      <td>0.298599</td>\n",
       "      <td>810.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>05-Mar-2018</td>\n",
       "      <td>2018-03-05 15:30:00</td>\n",
       "      <td>19886</td>\n",
       "      <td>0.626232</td>\n",
       "      <td>930.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>09-Mar-2018</td>\n",
       "      <td>2018-03-09 23:30:00</td>\n",
       "      <td>2505</td>\n",
       "      <td>0.078885</td>\n",
       "      <td>1410.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>05-Mar-2018</td>\n",
       "      <td>2018-03-05 05:30:00</td>\n",
       "      <td>2685</td>\n",
       "      <td>0.084554</td>\n",
       "      <td>330.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>09-Mar-2018</td>\n",
       "      <td>2018-03-09 04:30:00</td>\n",
       "      <td>274</td>\n",
       "      <td>0.008629</td>\n",
       "      <td>270.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Day            DateTime   Freq  normFreq  Minutes\n",
       "14   04-Mar-2018 2018-03-04 11:00:00   5617  0.176886    660.0\n",
       "178  07-Mar-2018 2018-03-07 21:00:00   3861  0.121587   1260.0\n",
       "314  10-Mar-2018 2018-03-10 17:00:00   9324  0.293623   1020.0\n",
       "54   05-Mar-2018 2018-03-05 07:00:00  19634  0.618296    420.0\n",
       "215  08-Mar-2018 2018-03-08 15:30:00  20122  0.633664    930.0\n",
       "259  09-Mar-2018 2018-03-09 13:30:00   9482  0.298599    810.0\n",
       "71   05-Mar-2018 2018-03-05 15:30:00  19886  0.626232    930.0\n",
       "279  09-Mar-2018 2018-03-09 23:30:00   2505  0.078885   1410.0\n",
       "51   05-Mar-2018 2018-03-05 05:30:00   2685  0.084554    330.0\n",
       "241  09-Mar-2018 2018-03-09 04:30:00    274  0.008629    270.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grp.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Day', 'ParentRoute', 'newDateTime', 'CardID'], dtype='object')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
